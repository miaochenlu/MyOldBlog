---
title: "Neural Networks"
tags: ML&AI
key: page-NN


---





# <center>Neural Networks</center>

## 简单的神经网络

### 神经元模型

在生物体中，神经元是神经系统最基本的结构和功能单位。它能够接收信号，当产生神经冲动，当神经冲动达到一阈值时，神经元被激活，并将信号传递给其他神经元。

<center><img src="https://miaochenlu.github.io/picture/屏幕快照2019-08-16下午12.38.30.png" width="600" /></center>

基于生物的神经元,1943年,McCulloch 和 Pitts提出了第一个神经元的计算模型，McCulloch-Pitts Neuron
n个信号带权重传递到该神经元，神经元接收到的总输入值与阈值比较，通过激活函数产生输出。[2]

<center><img src="https://miaochenlu.github.io/picture/屏幕快照2019-08-16下午2.03.10.png" width="300" /></center>

#### 为什么需要激活函数

如果没有激活函数，那么输出将会是
$ y=\sum_{i=0}^{n}w_ix_i $，这是一个线性的函数，表达能力有限。
考虑分类器，将蓝色和红色两类分开，z左图况我们可以用线性模型做到
但是如果两类的分布是右图的情况，我们就需要引入非线性因素。而激活函数可以用来加入非线性因素。



<table border="0"><tr>
<td><img src="https://miaochenlu.github.io/picture/image-20190930180951806.png" width=300 align=center /></td>
<td><img src="https://miaochenlu.github.io/picture/image-20190930181024583.png" width=250 align=center /></td>
</tr></table>

以下提供一些常用的激活函数

- $ sigmoid(x)=\frac{e^x}{e^x+1} $
- $ tanh(x)=\frac{e^x-1}{e^x+1} $
- $ ReLu(x)=max(0,x) $
- $ leaky ReLu(x)=max(0,x)+\alpha min(0,x) $

<br/>

## 复杂神经网络

左图是一个2层神经网络（一个有4个神经元的隐藏层，一个有2个神经元的输出层）,右图是一个3层神经网络（2个有4个神经元的隐藏层以及一个输出层）。

<center> <img src="https://miaochenlu.github.io/picture/屏幕快照2019-08-16下午3.32.09.png" width=600 /></center>

在计算神经网络的层数时，输入层是不计数的。
神经元在层与层之间有连接，在层内没有连接。
输出层通常不用激活函数，因为激活函数会掩盖掉一些取值，比如使用$ ReLu(x) $，就不会得到小于0的输出。在以下内容中，暂不考虑输出层不使用激活函数的情况。

 两层神经网络的例子如下

 <center><img src="https://miaochenlu.github.io/picture/屏幕快照2019-08-17上午8.56.02.png" width=300 /></center>


$ a_i=\sum_{j=1}^{n}w_{ji}x_j +w_{0i} $
$ c=\sum_{j=1}^{m}v_jb_j+v_0 $
多层神经网络同理

<br/>

## 训练神经网络

训练神经网络过程中，训练的是神经元之间的连接权和每个神经元的阈值

### Back Propagation

训练过程中，我们根据每次输出的结果预期之间的误差来用梯度下降法来更新权值。这里我们使用均方误差。我们来看看反向传播算法:
以下暂不考虑输出层不使用激活函数的情况。激活函数为sigmoid函数
以上图所给神经网络为例，
给定一个训练集

$ {(\mathbf{x_1},z_1), (\mathbf{x_2},z_2),\cdots,(\mathbf{x_t},z_t)},\mathbf{x_i}\in \mathbb{R}^m, z_i\in \mathbb{R}  $

对于训练集中的一个样例$ (\mathbf{x_k},z_k) $,假设神经网络的输出为$ \tilde{z_k} $,那么均方误差$ E_k $为

 $$ E_k=(\tilde{z_k}-z_k)^2 $$

假设学习率为$ \alpha $，以隐藏层到输出层到权重$ v_j $为例，运用梯度下降法

$$ v_j=v_j-\Delta v_j $$
 $$ \Delta v_j=- \alpha \frac{\partial E_k}{\partial v_j} $$

接来下我们计算这个式子

$ v_j $影响到输出层神经元的输入 $ c $，继而对$ \tilde{z_k} $产生影响，进而影响$ E_k $，从后往前推导，可以得到

$$ E_k=(\tilde{z_k}-z_k)^2 $$
$$ \tilde{z_k}=f(c) $$
$$ c=\sum_{j=1}^{m}v_jb_j+v_0 $$

<br/>
我们有$ \frac{\partial E_k}{\partial v_j}= \frac{\partial E_k}{\partial \tilde{z_k}}\frac{\partial \tilde{z_k}}{\partial c}\frac{\partial c}{\partial v_j} $
因为$ c=\sum_{j=1}^{m}v_jb_j+v_0 $,所以$ \frac{\partial c}{\partial v_j} = b_j $
因为$ \tilde{z_k}=f(c) $,sigmoid函数求导$ f(x)=f(x)(1-f(x)) $,所以$ \frac{\partial \tilde{z_k}}{\partial c} = f(c)(1-f(c))=\tilde{z_k}(1-\tilde{z_k}) $

综上
$$ \frac{\partial E_k}{\partial v_j}= \frac{\partial E_k}{\partial \tilde{z_k}}\frac{\partial \tilde{z_k}}{\partial c}\frac{\partial c}{\partial v_j}=2\tilde{z_k}(1-\tilde{z_k})(\tilde{z_k}-z_k)b_j $$
因此

$$ \Delta v_j=- \alpha \frac{\partial E_k}{\partial v_j} =-2\alpha \tilde{z_k}(1-\tilde{z_k})(\tilde{z_k}-z_k)b_j  $$
<br/>
同样的，考虑更新$ w_{ij} $
$$ w_{ij}=w_{ij}-\frac{\partial E_k}{\partial w_{ij}} $$
误差反向传递得到  $ E_k=(\tilde{z_k}-z_k)^2 $
$$ \tilde{z_k}=f(c) $$
$$ c=\sum_{j=0}^{m}v_jb_j $$
$$ b_j=f(a_j) $$
$$ a_j=\sum_{t=0}^{n}w_{tj}x_t $$

<br/>
得到$ \frac{\partial E_k}{\partial w_{ij}}=\frac{\partial E_k}{\partial \tilde{z_k}}\frac{\partial \tilde{z_k}}{\partial c}\frac{\partial c}{\partial b_j}\frac{\partial b_j}{\partial a_j}\frac{\partial a_j}{\partial w_{ij}}=2\tilde{z_k}(1-\tilde{z_k})(\tilde{z_k}-z_k)v_jb_j(1-b_j)x_i $

<br/>

## Scaling with Data

## 卷积神经网络

卷积神经网络与前面与提供的神经网络非常相似，不同的是卷积神经网络的输入是图片。
在处理图像上，常规神经网络的效果是不好的。如前所述，神经网络学习的是权重，大尺寸的图像会使所要学习的权重急剧增加，这样不仅学习效率低，还会过拟合。
卷积神经网络主要有三种类型的层：

- 卷积层
- 池化层
- 全连接层

### 卷积

卷积的本质是一种加权叠加
我们称 $ (f*g)(n) $为 $ f,g $的卷积
一维连续卷积的定义为
$ (f*g)(n)=\int_{-\infty}^{\infty}f(\tau)g(n-\tau)d\tau $
一维离散卷积的定义为
$ (f*g)(n)=\sum_{\tau=-\infty}^{\infty}f(\tau)g(n-\tau) $
其中$ f $称为输入，$ w $称为核函数

处理二维图像时，我们需要二维卷积核K
$ (I*K)(i,j)=\sum_m\sum_nI(m,n)K(i-m,j-n) $
若我们不翻转卷积核，效果是一样的，称为互相关函数
$ (I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n) $

如下图所示

<center><img src="https://miaochenlu.github.io/picture/屏幕快照2019-08-17下午7.41.00.png" width="600" /></center>



### 池化

**池化**也称为**欠采样**或**下采样**。主要用于特征降维，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。[4]
池化函数会得到代表相邻矩形区域的值，比如相邻矩形区域里的最大值、平均值......
如下图所示，$ 4\times 4 $的特征图像在最大池化后压缩成了$ 2\times 2 $的图像，达到了压缩数据的目的。

<center> <img src="https://miaochenlu.github.io/picture/屏幕快照2019-08-17下午8.10.46.png" width="400" /></center>

### 几种CNN结构

- **LeNet**

 <center><img src="https://miaochenlu.github.io/picture/屏幕快照2019-08-17下午9.02.48.png" width="500" /></center>


- **AlexNet**
- **GooleNet**
- **VGGNet**
- **ResNet**
- ...

References:
[1]https://zh.wikipedia.org/wiki/%E7%A5%9E%E7%B6%93%E5%85%83
[2]周志华.机器学习[M].97页
[3]http://cs231n.github.io/neural-networks-1/
[4]https://www.cnblogs.com/wj-1314/p/9593364.html

