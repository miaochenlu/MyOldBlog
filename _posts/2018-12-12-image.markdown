---

layout: post

title: "Image Processing"

date: 2018-12-12 12:21:05 +0800

categories: jekyll update

---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
inlineMath: [['$','$']]
}
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>



## **Basic Concept**
### Lens

<img src="http://miaochenlu.github.io/picture/picture20181222lens.png" width = "600" align = "center" id = "start">

A lens focuses light onto the film   
1. There is a specific distance at which objects are "in
focus"    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  other points project to a "circle of confusion" in the image   
2. Changing the shape of the lens changes this distance

### aperture(光圈)

<table border="0"><tr>
<td><img src="http:///miaochenlu.github.io/picture/picture20181222aperture.png" width = "600" border="0" ></td>
<td><p>
<b>Why not make the aperture as small as possible?</b> <br>
* The quantity of light is too small. <br>
* Lead to diffraction if the aperture is too small.  <br>
* Difficult to control.
</p></td>
</tr></table>


### depth of field(景深)
<img src = "http://miaochenlu.github.io/picture/picture20181222depth.png" width = "600" align = "center">

It can be seen from the picture that changing the aperture size affects depth of field.  
– A smaller aperture increases the range in which the
object is approximately in focus
  
* 镜头光圈：光圈越大，景深越小；光圈越小，景深越大；  
* 镜头焦距镜头焦距越长，景深越小；焦距越短，景深越大;  
    * 一般说来，在同样的光圈下，焦距越长的镜头其景深就越小，相反则越大。所以广角镜头有很大的景深，超广角镜头在其最大光圈下几厘米外都会有清晰的成像，但长焦镜头或望远镜头则景深很小，有时仅是几厘米景深，拍人像时弄不好就会出现一只眼睛是清晰的而另一只眼睛则虚化了
* 拍摄距离距离越远，景深越大；距离越近，景深越小。 
    * 而当拍摄时的光圈大小不变，所使用的镜头焦距也不改变时，被摄体越远，画面中的前后清晰范围就越大；反之，被摄体越近，前后的清晰范围也就相对越小。   
   
<a href="http://www.360doc.com/content/18/0104/17/50354283_719051589.shtml">景深</a>

### The principle imaging process of DC(digital camera)  
<img src="http://miaochenlu.github.io/picture/picture20181222dc.png" width = "1600" border="0" width = "600" align = "center">  

(CCD:可以称为CCD图像传感器,也叫图像控制器。CCD是一种半导体器件,能够把光学影像转化为电信号)  
 （1）When taking photograph, the light from the scene goes through the lens and
reaches CCD.  
 （2）When CCD being exposed, the photodiode is stimulated to release charges,
and produces electrical signals.  
 （3）CCD controlling chip controls the electric flow through the controlling signal
circuit in photoperceptive component. CCD will collect such electrical signals and
output them to an amplifier.  
 （4）After amplifying and filtering, the electrical signals reach ADC. And ADC
transfer such electrical signals (continuous) to digital ones (discrete). The value of digital signal is proportional to the intensity of electrical signal and voltage. These values are corresponding to those of an image.  
 （5）But the above data cannot be treated as image directly. They will be further
processed by DSP (digital signal processing). In DSP, color correction and white
balance will be performed to obtain a qualified image, and the image will be
encoded into the supported format and resolution, which can be stored as a image
file.  
 （6）After the above steps, the image file appears on your memory card and can
be previewed.  
  
##### 总而言之    
光-->CCD转化成电信号-->电信号放大过滤-->ADC将连续信号转换为离散信号-->DSP(digital signal processing)color correction and white balance-->image will be encoded into the supported format and resolution-->image file appears on your memory card   
## Color Space
### RGB color model
 RGB color model is a unit cube in a Cartesian coordinates system.  
▪ The magnitudes of each primary color are equivalent on the main diagonal line, which lead to the white color from darkness to brightness, i.e., grayscale.  
▪ (0,0,0)-dark, (1,1,1)-bright. The other 6 corners are respectively <span style="color:red;">red</span>, <span style="color:yellow;">yellow </span>, <span style="color:cyan;">cyan </span>,<span style="color:blue;">blue </span> and <span style="color:magenta;">magenta </span>.  
▪ RGB is a subset of CIE primary color space.  
▪ RGB is usually used in <i>Color cathode ray tube and Color raster graphics display
(computer , TV).</i> 


## Otsu 大津算法
Otsu算法也称最大类间差法  
* 思想  
按图像的灰度特性将图像分成 **背景**和 **前景**，前景和背景的 **类间方差**最大，说明构成图像的两部分差别越大，当部分前景错分为背景或部分背景错分为前景都会导致两部分差别变小。

* 推导  
<img src="http://miaochenlu.github.io/picture/picture20190109otsudeduct.png">


```c
int Otsu(U8*pData, U8 bitCountPerPix, U32 width, U32 height) {
    int Pn[256]={0};
    U32 bmppitch = ((width*bitCountPerPix + 31) >> 5) << 2;
    U32 filesize = bmppitch*height;
    U8 BytePerPix = bitCountPerPix >> 3;
    U32 pitch = width * BytePerPix;
    int h,w;
    for(h = height-1; h >= 0; h--) {
        for(w = 0; w < width; w++) {
            Pn[pData[h*pitch + w*BytePerPix + 0]]++;
        }
        
    }
    U32 pixelCount = width * height;
    U32  NumberSmall = 0;
    U32  NumberBig = 0;
    double MaxGapSquare = 0;
    double TmpGapSquare = 0;
    U32 m1k = 0;
    U32 m2k = 0;
    U32 mgk = 0;
    U32 MaxK = 0;
    U32 k;
    for(k = 0; k < 256; k++) {
        NumberSmall = 0;
        NumberBig = 0;
        TmpGapSquare = 0;
        m1k = 0;
        m2k = 0;
        mgk = 0;
        for(int i = 0; i <=k; i++) {
            NumberSmall += Pn[i];
            m1k += i * Pn[i];
        }
        for(int j = k+1; j < 256; j++) {
            NumberBig +=Pn[j];
            m2k += j * Pn[j];
        }
        mgk = m1k + m2k;
        TmpGapSquare = ((mgk * 1.0 / pixelCount) * (NumberSmall * 1.0 / pixelCount) - m1k * 1.0 / pixelCount) *
                        ((mgk * 1.0 / pixelCount) * (NumberSmall * 1.0 / pixelCount) - m1k * 1.0 / pixelCount) /
                        ((NumberSmall * 1.0 / pixelCount) * (1 - NumberSmall * 1.0 / pixelCount));
        if(TmpGapSquare > MaxGapSquare) 
        {
            MaxGapSquare = TmpGapSquare;
            MaxK = k;
        }
    }
    return MaxK;
}
```
## **Spatial Filtering**
### **Linear smoothing filter**
* **Concept**    
The output of the linear smoothing filter is the mean value of the pixels in the mask. It’s also called mean filter.
* **application**   
Mean filter is mainly used for subtle detail removal,
namely, eliminating the unwanted region smaller than the mask.  
* **general equation**     

$$
    g(x,y)=\frac{\Sigma_{s=-a}^{s=a}\Sigma_{t=-b}^{t=b}w(s,t)f(x+s,y+t)}{\Sigma_{s=-a}^{s=a}\Sigma_{t=-b}^{t=b}w(s,t)}
$$

<table border="0"><tr>
<td><img src="http://miaochenlu.github.io/picture/picture20181210nonweight.png" width = "300" border="0" ></td>
<td><img src="http://miaochenlu.github.io/picture/picture20181210weight.png" width = "300" border="0"></td>
</tr></table>

### **Statistical sorting filter**
* **Concept**  
Statistical filter is a kind of nonlinear spatial filter, whose response is based on the **sorting of pixel value in the mask window**.The value of center pixel depends on the sorting result in the window.  
The most popular statistical filter is median filter.
* **Median filter**  
    * Subsititute the center pixel with the median value in the neighborhood.  
    * Provide excellent de-noise ability, which introduce less blurring than the mean filter.
    * Be effective to deal with the pulse noise(or pepper noise) because this kind of noise looks like bright or dark point in the image.  
* **example**
In a $3 \times 3$ neighborhood, there are a series pixel values:  
(21, 100, 99, 22, 20, 102, 97, 101)  
After sorting:  
(20, 21, 22, 97, 100, 101, 102)  
<img src="http://miaochenlu.github.io/picture/picture20191217sortfilter.png">  

### **Bilateral filter**
* **Gereral idea** 
An image has two main characteristics
    * The space domain S, which is the set of possible
    positions in an image. This is related to the
    resolution, i.e., the number of rows and columns in
    the image.
    * The intensity domain R, which is the set of
    possible pixel values. The number of bits used to
    represent the pixel value may vary. Common pixel
    representations are unsigned bytes (0 to 255) and
    floating point  

Every sample is replaced by a weighted average of its neighbors,  These weights reflect two forces
* How close are the neighbor and the center sample, so that
larger weight to closer samples,
* How similar are the neighbor and the center sample –
larger weight to similar samples.   
All the weights should be normalized to preserve the
local mean.

```c
void Bilateral(U8 *pdata, U8 bitCountPerPix, U32 width, U32 height,double sigma_s, double sigma_r, const char *filename)
{
    int i, j;
    int maskh,maskw;
    U8 BytePerPix = bitCountPerPix >> 3;
    U32 pitch = width * BytePerPix;

    U32 bmppitch = ((width * bitCountPerPix + 31) >> 5) << 2;
    U8 *copypdata = (U8*)malloc(height*bmppitch);
    memset(copypdata, 0, bmppitch*height);
    memcpy(copypdata, pdata, height*bmppitch);

    double wred, wblue, wgreen;
    double ws;
    double wr_green, wr_blue, wr_red;
    double pixelgreen, pixelblue, pixelred;
    for(j = height - 22; j >= 20; j--) {
        for(i = 20; i <= width - 22; i++) {
            wred = 0; wblue = 0; wgreen = 0; 
            ws = 0; 
            wr_green = 0; wr_blue = 0; wr_red = 0;
            pixelgreen = 0; pixelblue = 0; pixelred = 0;

            for(maskh = j + 21; maskh >= j - 20; maskh--) {
                for(maskw = i - 20; maskw <= i + 21; maskw++) {
                    ws = exp(-((maskh-j)*(maskh-j)+(maskw-i)*(maskw-i))/(2*sigma_s*sigma_s));
                    double differencegreen = (pdata[maskh*pitch + maskw*BytePerPix + 0]-pdata[j*pitch + i*BytePerPix + 0]);
                    double differenceblue = (pdata[maskh*pitch + maskw*BytePerPix + 1]-pdata[j*pitch + i*BytePerPix + 1]);
                    double differencered = (pdata[maskh*pitch + maskw*BytePerPix + 2]-pdata[j*pitch + i*BytePerPix + 2]);
                    wr_green = exp(-(differencegreen*differencegreen)/(2*sigma_r*sigma_r));
                    wr_blue = exp(-(differenceblue*differenceblue)/(2*sigma_r*sigma_r));
                    wr_red = exp(-(differencered*differencered)/(2*sigma_r*sigma_r));
                    wgreen += ws * wr_green;
                    wblue += ws * wr_blue;
                    wred += ws * wr_red;
                    pixelgreen += wr_green * ws * pdata[maskh*pitch + maskw*BytePerPix + 0];
                    pixelblue += wr_blue * ws * pdata[maskh*pitch + maskw*BytePerPix + 1];
                    pixelred += wr_red * ws * pdata[maskh*pitch + maskw*BytePerPix + 2];
                }
            }
            copypdata[j*pitch + i*BytePerPix + 0] = pixelgreen / wgreen;
            copypdata[j*pitch + i*BytePerPix + 1] = pixelblue / wblue;
            copypdata[j*pitch + i*BytePerPix + 2] = pixelred / wred;

        }
    }
 
    GenerateBmpFile(copypdata,bitCountPerPix, width, height, filename);
    free(copypdata);
}
```

Result:  
<img src="http://miaochenlu.github.io/picture/picture20181222bila.png">

## **图像插值算法：**  
### 1、最邻近插值  
为了计算⼏何变换后新图像中某⼀点P’处的像
素值，可以⾸先计算该⼏何变换的逆变换，计算出
P’所对应的原图像中的位置P。通常情况下，P的位
置不可能正好处在原图像的某⼀个像素位置上（即
P点的坐标通常都不会正好是整数）。寻找与P点最
接近的像素Q，把Q点的像素值作为新图像中P’点的
像素值。

<img src="http://miaochenlu.github.io/picture/picture20181128interpolation.png">

### 2、线性插值
<!-- $$g_3 = \frac{g_2-g_1}{x_2-x_1}(x_3-x_1)+g_1$$    -->

<table border="0"><tr>
<td><img src="http://miaochenlu.github.io/picture/picture20181128linear.png" width = "400" border="0"></td>
<td><img src="http://miaochenlu.github.io/picture/picture20181128 linearequation.png" width = "200" border="0"></td>
</tr></table>


### 3、双线性插值
<img src="http://miaochenlu.github.io/picture/picture20181205bilinear.png">
<img src="http://miaochenlu.github.io/picture/picture20181205bilinearequa.png">

## Simple geometric transformation
### 1、Translation 平移 

<img src="http://miaochenlu.github.io/picture/picture20181205translation.png" width = "300">  

其逆变换为
<img src="http://miaochenlu.github.io/picture/picture2018translationreverse.png">  


根据其逆变换，我们可以知道平移后的图像中的像素对应的在原来图像中的坐标


```c
void translation(U8* pdata,U32 bitCountPerPix,int width,int height,int xoffbit,int yoffbit,const char*filename )
{
    int translationwidth = width + abs(xoffbit);
    int translationheight = height + abs(yoffbit);
    U32 bmppitch = ((translationwidth*bitCountPerPix + 31) >> 5) << 2;
    U8 BytePerPix = bitCountPerPix >> 3;
    U8* copydata = (U8*)malloc(bmppitch*translationheight);
    memset(copydata, 255, bmppitch*translationheight);
    U32 originpitch = width * BytePerPix;
    U32 pitch = translationwidth * BytePerPix;
    int h,w;
    for(h = translationheight - 1; h >=yoffbit ; h--) {
        for(w = xoffbit; w < translationwidth; w++) {
            copydata[h*pitch+w*BytePerPix+0] = pdata[(h-yoffbit)*originpitch+(w-xoffbit)*BytePerPix+0];
            copydata[h*pitch+w*BytePerPix+1] = pdata[(h-yoffbit)*originpitch+(w-xoffbit)*BytePerPix+1];
            copydata[h*pitch+w*BytePerPix+2] = pdata[(h-yoffbit)*originpitch+(w-xoffbit)*BytePerPix+2];
        }
    }
    GenerateBmpFile(copydata,bitCountPerPix,translationwidth,translationheight,filename);
    free(copydata);
}
```

<a id="bottomsection" href="#start">去顶部</a>

[jekyll-docs]: https://jekyllrb.com/docs/home

[jekyll-gh]: https://github.com/jekyll/jekyll

[jekyll-talk]: https://talk.jekyllrb.com/
