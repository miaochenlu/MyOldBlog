---

layout: post

title: "Image Processing"

date: 2018-12-12 12:21:05 +0800

categories: jekyll update

---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
inlineMath: [['$','$']]
}
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>



# **Basic Concept**
### Lens

<img src="http://miaochenlu.github.io/picture/picture20181222lens.png" width = "600" align = "center" id = "start">

A lens focuses light onto the film   
1. There is a specific distance at which objects are "in
focus"    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  other points project to a "circle of confusion" in the image   
2. Changing the shape of the lens changes this distance

### aperture(光圈)

<table border="0"><tr>
<td><img src="http:///miaochenlu.github.io/picture/picture20181222aperture.png" width = "600" border="0" ></td>
<td><p>
<b>Why not make the aperture as small as possible?</b> <br>
* The quantity of light is too small. <br>
* Lead to diffraction if the aperture is too small.  <br>
* Difficult to control.
</p></td>
</tr></table>


### depth of field(景深)
<img src = "http://miaochenlu.github.io/picture/picture20181222depth.png" width = "600" align = "center">

It can be seen from the picture that changing the aperture size affects depth of field.  
– A smaller aperture increases the range in which the
object is approximately in focus
  
* 镜头光圈：光圈越大，景深越小；光圈越小，景深越大；  
* 镜头焦距镜头焦距越长，景深越小；焦距越短，景深越大;  
    * 一般说来，在同样的光圈下，焦距越长的镜头其景深就越小，相反则越大。所以广角镜头有很大的景深，超广角镜头在其最大光圈下几厘米外都会有清晰的成像，但长焦镜头或望远镜头则景深很小，有时仅是几厘米景深，拍人像时弄不好就会出现一只眼睛是清晰的而另一只眼睛则虚化了
* 拍摄距离距离越远，景深越大；距离越近，景深越小。 
    * 而当拍摄时的光圈大小不变，所使用的镜头焦距也不改变时，被摄体越远，画面中的前后清晰范围就越大；反之，被摄体越近，前后的清晰范围也就相对越小。   
   
<a href="http://www.360doc.com/content/18/0104/17/50354283_719051589.shtml">景深</a>

### The principle imaging process of DC(digital camera)  
<img src="http://miaochenlu.github.io/picture/picture20181222dc.png" width = "1600" border="0" width = "600" align = "center">  

(CCD:可以称为CCD图像传感器,也叫图像控制器。CCD是一种半导体器件,能够把光学影像转化为电信号)  
 （1）When taking photograph, the light from the scene goes through the lens and
reaches CCD.  
 （2）When CCD being exposed, the photodiode is stimulated to release charges,
and produces electrical signals.  
 （3）CCD controlling chip controls the electric flow through the controlling signal
circuit in photoperceptive component. CCD will collect such electrical signals and
output them to an amplifier.  
 （4）After amplifying and filtering, the electrical signals reach ADC. And ADC
transfer such electrical signals (continuous) to digital ones (discrete). The value of digital signal is proportional to the intensity of electrical signal and voltage. These values are corresponding to those of an image.  
 （5）But the above data cannot be treated as image directly. They will be further
processed by DSP (digital signal processing). In DSP, color correction and white
balance will be performed to obtain a qualified image, and the image will be
encoded into the supported format and resolution, which can be stored as a image
file.  
 （6）After the above steps, the image file appears on your memory card and can
be previewed.  
  
#### 总而言之    
光-->CCD转化成电信号-->电信号放大过滤-->ADC将连续信号转换为离散信号-->DSP(digital signal processing)color correction and white balance-->image will be encoded into the supported format and resolution-->image file appears on your memory card  

### 颜色分类
* chromatic color 
    * monochrome color (red, yellow, blue, etc.) and their combination. The colorful objects reflect the light selectively to the light spectrum of different wavelengths. Hence, they present different color under white light.
* achromatic color 
    * Achromatic color, also called non- chromatic color OR grayscale, means white, black and the gray intensities between them. The achromatic objects do not select the wavelength, so they are in neutral color.

### Weber's Law
<img src="http://miaochenlu.github.io/picture/20190122pictureweberslaw.jpeg">  
The response of the eye to changes in the intensity of illumination is known to be nonlinear. Consider a patch of light of intensity I + ΔI surrounded by a background of intensity I . The just noticeable difference ΔI is to be determined as a function of I. Over a wide range of intensities, it is found that the ratio ΔI ⁄ I , called the Weber fraction, is nearly constant at a value of about 0.02 . This result does not hold at very low or very high intensities. Furthermore, contrast sensitivity is dependent on the intensity of the surround. Consider the experiment of Figure 2.3-1b, in which two patches of light, one of intensity I and the other of intensity I + ΔI , are surrounded by light of intensityIo . The Weber fraction ΔI ⁄ I for this experiment is plotted in Figure 2.3-1b as a function of the intensity of the background. In this situation, it is found that the range over which the Weber fraction remains constant is reduced con- siderably compared to the experiment. The envelope of the lower limits of the curves of Figure 2.3-lb is equivalent to the curve of Figure 2.3-1a. However, the range over which ΔI ⁄ I is approximately constant for a fixed background intensity Io is still comparable to the dynamic range of most electronic imaging systems.  
# **Color Space**
* 设备相关
    * RGB
    * CMY
    * HSV
* 设备无关
    * CIE XYZ
    * CIE L*a*b
    * CIE YUV
### RGB color model
 RGB color model is a unit cube in a Cartesian coordinates system.  
▪ The magnitudes of each primary color are equivalent on the main diagonal line, which lead to the white color from darkness to brightness, i.e., grayscale.  
▪ (0,0,0)-dark, (1,1,1)-bright. The other 6 corners are respectively <span style="color:red;">red</span>, <span style="color:yellow;">yellow </span>, <span style="color:cyan;">cyan </span>,<span style="color:blue;">blue </span> and <span style="color:magenta;">magenta </span>.  
▪ RGB is a subset of CIE primary color space.  
▪ RGB is usually used in <i>Color cathode ray tube and Color raster graphics display
(computer , TV).</i> 

是加色，加光 color + light energy increase  
### CMY 
C 青色 M 品红 Y 黄色 (K 黑色)  
减色空间 light - pigment  energy decreases
<img src="http://miaochenlu.github.io/picture/20190122picturecmyk.png">
<img src="http://miaochenlu.github.io/picture/20190122pictureaddcolor.png">
### HSV 
H 色调 S 饱和度 V亮度
*perception priority 
    HSV
* sentivity 
    lightness  
because   
Rods: ~100, 000, 000, sensitive to light but cannot identify different colors.  
Cones: ~6,000,000-7,000,000, work under strong light, but can identify different colors.

### YUV
Y 明亮度  U 色度  V 浓度

# **Image file**
### BMP 
位图数据：位图数据在文件中的排列顺序是从左下角到右上角，以行为主序排列的。  
对齐规则：要求每行的数据的长度必须是4的倍数，如果不够需要进行比特填充（以0填充）
<img src="http://miaochenlu.github.io/picture/20190122picturebitmapdata.png">  
<a href="http://www.cnblogs.com/wainiwann/p/7086844.html">BMP格式详解</a>

### 图像压缩
Compression strategy: According to the requirement of compressionratio, remove information from high frequency to low frequency. Advantages 

* 无损
    * bmp 支持run length code(RLE)
    * png
* 有损 
    * jpeg(DCT算法 Discrete Cosine Transformation )， gif
* 均可 
    * tiff

### run length code
<img src="http://miaochenlu.github.io/picture/20190122picturerunlengthcode.png">

## Binary Image
<img src="http://miaochenlu.github.io/picture/20190122picturebinary.png">

how to choose threshold? -->otsu
### Otsu 大津算法
Otsu算法也称最大类间差法  
* 思想  
按图像的灰度特性将图像分成 **背景**和 **前景**，前景和背景的 **类间方差**最大，说明构成图像的两部分差别越大，当部分前景错分为背景或部分背景错分为前景都会导致两部分差别变小。

* 推导  
<img src="http://miaochenlu.github.io/picture/picture20190109otsudeduct.png">
easy approach
<img src="http://miaochenlu.github.io/picture/20190122pictureotsueasy.png">
**local adaptive operation**
* set a local window, find its good threshold;
* sliding your local window over the whole image
```c
int Otsu(U8*pData, U8 bitCountPerPix, U32 width, U32 height) {
    int Pn[256]={0};
    U32 bmppitch = ((width*bitCountPerPix + 31) >> 5) << 2;
    U32 filesize = bmppitch*height;
    U8 BytePerPix = bitCountPerPix >> 3;
    U32 pitch = width * BytePerPix;
    int h,w;
    for(h = height-1; h >= 0; h--) {
        for(w = 0; w < width; w++) {
            Pn[pData[h*pitch + w*BytePerPix + 0]]++;
        }
        
    }
    U32 pixelCount = width * height;
    U32  NumberSmall = 0;
    U32  NumberBig = 0;
    double MaxGapSquare = 0;
    double TmpGapSquare = 0;
    U32 m1k = 0;
    U32 m2k = 0;
    U32 mgk = 0;
    U32 MaxK = 0;
    U32 k;
    for(k = 0; k < 256; k++) {
        NumberSmall = 0;
        NumberBig = 0;
        TmpGapSquare = 0;
        m1k = 0;
        m2k = 0;
        mgk = 0;
        for(int i = 0; i <=k; i++) {
            NumberSmall += Pn[i];
            m1k += i * Pn[i];
        }
        for(int j = k+1; j < 256; j++) {
            NumberBig +=Pn[j];
            m2k += j * Pn[j];
        }
        mgk = m1k + m2k;
        TmpGapSquare = ((mgk * 1.0 / pixelCount) * (NumberSmall * 1.0 / pixelCount) - m1k * 1.0 / pixelCount) *
                        ((mgk * 1.0 / pixelCount) * (NumberSmall * 1.0 / pixelCount) - m1k * 1.0 / pixelCount) /
                        ((NumberSmall * 1.0 / pixelCount) * (1 - NumberSmall * 1.0 / pixelCount));
        if(TmpGapSquare > MaxGapSquare) 
        {
            MaxGapSquare = TmpGapSquare;
            MaxK = k;
        }
    }
    return MaxK;
}
```
### Morphology
* **Dilation**
    * **definition**
        <img src="http://miaochenlu.github.io/picture/20190122picturedilation.png">
        范围里有1就是1
    * **physical meaning**:  
        Dilation adopts the connected background pixels into the foreground, which extends its boundary and fill the holes in the foreground.And whether “connected” is decided by the structure element.
    * **Effect**:
        enlarge foreground, fill holes
    * **Example**
        <img src="http://miaochenlu.github.io/picture/20190122picturedilaexample.png">
* **Erosion**
    * **definition**
        <img src="http://miaochenlu.github.io/picture/20190122pictureerosion.png">
        范围里有0就是0
    * **physical meaning**:  
       remove boundary, remove unwanted small objects
    * **Example**
        <img src="http://miaochenlu.github.io/picture/20190122pictureeroexample.png">
* **Opening**
    * **definition**
        <img src="http://miaochenlu.github.io/picture/20190122pictureopen.png">
        先腐蚀，再膨胀
    * **physical meaning**:  
       Remove small objects, segment object at thin part, smooth boundary of large object but preserve its original area.
* **closing**
* **definition**
        <img src="http://miaochenlu.github.io/picture/20190122pictureclose.png">
        先膨胀，再腐蚀
    * **physical meaning**:  
        Fill small holes, connect the neighboring objects, smooth boundary while preserving the area at most.
<img src="http://miaochenlu.github.io/picture/20190122pictureopenclose.png">

## Grayscale Transform
### Grayscale perception
<img src="http://miaochenlu.github.io/picture/20190122picturegrayscale.png">
<img src="http://miaochenlu.github.io/picture/20190122grayscaleweber.png">
255次是指每一个灰度级要被看见的话，需要和前面相邻的灰度级相差Kweber

### logarithmic operation
图像的低灰度值部分扩展，显示出低灰度部分更多的细节，将其高灰度值部分压缩  
<img src="http://miaochenlu.github.io/picture/20190123log.png">

## Image Morph
* difference between morph and warp
    * Morph is not warp (geometric transformation)
    * Morph is a kind of morphological changing, which makes an image change to another image gradually.
    * 􏰀Morph handles both the location and the intensity of a pixel.
    * 􏰀The beginning image and end image are two key frames. Other frames between the two key frames are generated automatically.
<img src="http://miaochenlu.github.io/picture/20190122morph.png">
* result
<img src="http://miaochenlu.github.io/picture/20190123morphresult.png">

## **Expressive Expression Mapping with Ratio Images**
* **problem**
    <img src="http://miaochenlu.github.io/picture/20190122expe.png" height=300>
* **one possible solution**
    <img src="http://Desktop/miaochenlu.github.io/picture/20190122expresolu1.png">
* **Ratio Images**
    * **Lambertian model**
    <img src="http://miaochenlu.github.io/picture/20190122lambertian.png">
    参数$$\rou:材质 I_i:光照 n:表面的单位法向量 l_i:点光源的单位方向向量$$  
    
    <img src="http://miaochenlu.github.io/picture/20190122lambertianexplain.png>

## **Spatial Filtering**
### **Linear smoothing filter**
* **Concept**    
The output of the linear smoothing filter is the mean value of the pixels in the mask. It’s also called mean filter.
* **application**   
Mean filter is mainly used for subtle detail removal,
namely, eliminating the unwanted region smaller than the mask.  
* **general equation**     

$$
    g(x,y)=\frac{\Sigma_{s=-a}^{s=a}\Sigma_{t=-b}^{t=b}w(s,t)f(x+s,y+t)}{\Sigma_{s=-a}^{s=a}\Sigma_{t=-b}^{t=b}w(s,t)}
$$

<table border="0"><tr>
<td><img src="http://miaochenlu.github.io/picture/picture20181210nonweight.png" width = "300" border="0" ></td>
<td><img src="http://miaochenlu.github.io/picture/picture20181210weight.png" width = "300" border="0"></td>
</tr></table>

### **Statistical sorting filter**
* **Concept**  
Statistical filter is a kind of nonlinear spatial filter, whose response is based on the **sorting of pixel value in the mask window**.The value of center pixel depends on the sorting result in the window.  
The most popular statistical filter is median filter.
* **Median filter**  
    * Subsititute the center pixel with the median value in the neighborhood.  
    * Provide excellent de-noise ability, which introduce less blurring than the mean filter.
    * Be effective to deal with the pulse noise(or pepper noise) because this kind of noise looks like bright or dark point in the image.  
* **example**
In a $3 \times 3$ neighborhood, there are a series pixel values:  
(21, 100, 99, 22, 20, 102, 97, 101)  
After sorting:  
(20, 21, 22, 97, 100, 101, 102)  
<img src="http://miaochenlu.github.io/picture/picture20191217sortfilter.png">  

### **Bilateral filter**
* **Gereral idea** 
An image has two main characteristics
    * The space domain S, which is the set of possible
    positions in an image. This is related to the
    resolution, i.e., the number of rows and columns in
    the image.
    * The intensity domain R, which is the set of
    possible pixel values. The number of bits used to
    represent the pixel value may vary. Common pixel
    representations are unsigned bytes (0 to 255) and
    floating point  

Every sample is replaced by a weighted average of its neighbors,  These weights reflect two forces
* How close are the neighbor and the center sample, so that
larger weight to closer samples,
* How similar are the neighbor and the center sample –
larger weight to similar samples.   
All the weights should be normalized to preserve the
local mean.

```c
void Bilateral(U8 *pdata, U8 bitCountPerPix, U32 width, U32 height,double sigma_s, double sigma_r, const char *filename)
{
    int i, j;
    int maskh,maskw;
    U8 BytePerPix = bitCountPerPix >> 3;
    U32 pitch = width * BytePerPix;

    U32 bmppitch = ((width * bitCountPerPix + 31) >> 5) << 2;
    U8 *copypdata = (U8*)malloc(height*bmppitch);
    memset(copypdata, 0, bmppitch*height);
    memcpy(copypdata, pdata, height*bmppitch);

    double wred, wblue, wgreen;
    double ws;
    double wr_green, wr_blue, wr_red;
    double pixelgreen, pixelblue, pixelred;
    for(j = height - 22; j >= 20; j--) {
        for(i = 20; i <= width - 22; i++) {
            wred = 0; wblue = 0; wgreen = 0; 
            ws = 0; 
            wr_green = 0; wr_blue = 0; wr_red = 0;
            pixelgreen = 0; pixelblue = 0; pixelred = 0;

            for(maskh = j + 21; maskh >= j - 20; maskh--) {
                for(maskw = i - 20; maskw <= i + 21; maskw++) {
                    ws = exp(-((maskh-j)*(maskh-j)+(maskw-i)*(maskw-i))/(2*sigma_s*sigma_s));
                    double differencegreen = (pdata[maskh*pitch + maskw*BytePerPix + 0]-pdata[j*pitch + i*BytePerPix + 0]);
                    double differenceblue = (pdata[maskh*pitch + maskw*BytePerPix + 1]-pdata[j*pitch + i*BytePerPix + 1]);
                    double differencered = (pdata[maskh*pitch + maskw*BytePerPix + 2]-pdata[j*pitch + i*BytePerPix + 2]);
                    wr_green = exp(-(differencegreen*differencegreen)/(2*sigma_r*sigma_r));
                    wr_blue = exp(-(differenceblue*differenceblue)/(2*sigma_r*sigma_r));
                    wr_red = exp(-(differencered*differencered)/(2*sigma_r*sigma_r));
                    wgreen += ws * wr_green;
                    wblue += ws * wr_blue;
                    wred += ws * wr_red;
                    pixelgreen += wr_green * ws * pdata[maskh*pitch + maskw*BytePerPix + 0];
                    pixelblue += wr_blue * ws * pdata[maskh*pitch + maskw*BytePerPix + 1];
                    pixelred += wr_red * ws * pdata[maskh*pitch + maskw*BytePerPix + 2];
                }
            }
            copypdata[j*pitch + i*BytePerPix + 0] = pixelgreen / wgreen;
            copypdata[j*pitch + i*BytePerPix + 1] = pixelblue / wblue;
            copypdata[j*pitch + i*BytePerPix + 2] = pixelred / wred;

        }
    }
 
    GenerateBmpFile(copypdata,bitCountPerPix, width, height, filename);
    free(copypdata);
}
```

Result:  
<img src="http://miaochenlu.github.io/picture/picture20181222bila.png">

## **图像插值算法：**  
### 1、最邻近插值  
为了计算⼏何变换后新图像中某⼀点P’处的像
素值，可以⾸先计算该⼏何变换的逆变换，计算出
P’所对应的原图像中的位置P。通常情况下，P的位
置不可能正好处在原图像的某⼀个像素位置上（即
P点的坐标通常都不会正好是整数）。寻找与P点最
接近的像素Q，把Q点的像素值作为新图像中P’点的
像素值。

<img src="http://miaochenlu.github.io/picture/picture20181128interpolation.png">

### 2、线性插值
<!-- $$g_3 = \frac{g_2-g_1}{x_2-x_1}(x_3-x_1)+g_1$$    -->

<table border="0"><tr>
<td><img src="http://miaochenlu.github.io/picture/picture20181128linear.png" width = "400" border="0"></td>
<td><img src="http://miaochenlu.github.io/picture/picture20181128 linearequation.png" width = "200" border="0"></td>
</tr></table>


### 3、双线性插值
<img src="http://miaochenlu.github.io/picture/picture20181205bilinear.png">
<img src="http://miaochenlu.github.io/picture/picture20181205bilinearequa.png">

## Simple geometric transformation
### 1、Translation 平移 

<img src="http://miaochenlu.github.io/picture/picture20181205translation.png" width = "300">  

其逆变换为
<img src="http://miaochenlu.github.io/picture/picture2018translationreverse.png">  


根据其逆变换，我们可以知道平移后的图像中的像素对应的在原来图像中的坐标


```c
void translation(U8* pdata,U32 bitCountPerPix,int width,int height,int xoffbit,int yoffbit,const char*filename )
{
    int translationwidth = width + abs(xoffbit);
    int translationheight = height + abs(yoffbit);
    U32 bmppitch = ((translationwidth*bitCountPerPix + 31) >> 5) << 2;
    U8 BytePerPix = bitCountPerPix >> 3;
    U8* copydata = (U8*)malloc(bmppitch*translationheight);
    memset(copydata, 255, bmppitch*translationheight);
    U32 originpitch = width * BytePerPix;
    U32 pitch = translationwidth * BytePerPix;
    int h,w;
    for(h = translationheight - 1; h >=yoffbit ; h--) {
        for(w = xoffbit; w < translationwidth; w++) {
            copydata[h*pitch+w*BytePerPix+0] = pdata[(h-yoffbit)*originpitch+(w-xoffbit)*BytePerPix+0];
            copydata[h*pitch+w*BytePerPix+1] = pdata[(h-yoffbit)*originpitch+(w-xoffbit)*BytePerPix+1];
            copydata[h*pitch+w*BytePerPix+2] = pdata[(h-yoffbit)*originpitch+(w-xoffbit)*BytePerPix+2];
        }
    }
    GenerateBmpFile(copydata,bitCountPerPix,translationwidth,translationheight,filename);
    free(copydata);
}
```

<a id="bottomsection" href="#start">去顶部</a>

[jekyll-docs]: https://jekyllrb.com/docs/home

[jekyll-gh]: https://github.com/jekyll/jekyll

[jekyll-talk]: https://talk.jekyllrb.com/
