---
title: "Approximation Theory"
tags: Numarical_Analysis
key: page-NA8



---

<!--more-->



给定$x_1,\cdots,x_m$, 以及$y_1,\cdots,y_m$, 找一个更简单的函数$P(x)\approx f(x)$

但是

* m通常很大
* $y_i$是实验数据，并不精确，也就是说$y_i\not= f(x_i)$

我们的目标是找到best fit $P(x)$ 使得误差$P(x_i)-y_i$对所有数据点都小

Options:

* minimize $\underset{1\leq i\leq m}{max}\vert P(x_i)-y_i\vert$  minmax problem 最大的误差最小化(对应∞范数)
* minimize $\sum_{i=1}^m\vert P(x_i)-y_i\vert$ absolute deviation 对应L-1范数
* minimize $\sum_{i=1}^m \vert P(x_i)-y_i\vert ^2$ least-squares method 对应L-2范数



# 1. Discrete Least Squares Approximation

Determine the polynomial $P_n(x)=a_0+a_1x+\cdots+a_nx^n$ to approximate a set of data $\{(x_i,y_i)\vert i=1,2,\cdots,m\}$ such that least squares error $E_2=\sum_{i=1}^m[P_n(x_i)-y_i]^2$ is minimized.

Here, $n<<m$

事实上，$E_2$是$a_0,a_1,\cdots,a_n$的函数，即

$$E_2(a_0,a_1,\cdots,a_n)=\sum_{i=1}^m[a_0+a_1x_i+\cdots+a_nx_i^n-y_i]^2$$

要最小化$E_2$, 则要使$\frac{\partial E_2}{\partial a_k}=0,k=0,\cdots,n$

$$\begin{aligned}0&=\frac{\partial E_2}{\partial a_k}\\&=2\sum_{i=1}^m[P_n(x_i)-y_i]\cdot \frac{\partial P_n(x_i)}{\partial a_k}\\&=2\sum_{i=1}^m[\sum_{j=0}^na_jx_i^j-y_i]x_i^k\\&=2(\sum_{j=0}^na_j(\sum_{i=1}^mx_i^{j+k})-\sum_{i=1}^my_ix_i^k )\end{aligned}$$

令$b_k=\sum_{i=1}^m x_i^k, c_k=\sum_{i=1}^my_ix_i^k$

对所有$a_k$求导，写成矩阵形式

$$\left[\begin{matrix}b_{0+0}&\cdots&b_{0+n}\\\vdots&\vdots&\vdots\\ b_{n+0}&\cdots&b_{n+n} \end{matrix}\right]\left[\begin{matrix}a_0\\\vdots \\a_n\end{matrix}\right]=\left[\begin{matrix}c_0\\ \vdots\\ c_n\end{matrix}\right]$$



另一种推导方式

$$\begin{aligned}P(x)=&\alpha_0x^0+\alpha_1x^1+\cdots+\alpha_nx^n\\=&(x^0, x^1,\cdots,x^n)\left[\begin{matrix}\alpha_0\\\alpha_1\\\vdots\\\alpha_n\end{matrix}\right]  =(\phi(x))(a)\end{aligned}$$

$$E=e_i^2=\left\Vert\begin{matrix}e_1\\\cdots\\c_n\end{matrix}\right\Vert_2^2$$

$e_i=(\phi(x_i))(a)-y_i$

$$\left[\begin{matrix}e_1\\\cdots\\e_n\end{matrix}\right]=\left[\begin{matrix}\phi(x_1)\\\vdots\\ \phi(x_n)\end{matrix}\right] \left[\begin{matrix}a_0\\\vdots \\a_n \end{matrix}\right]-\left[\begin{matrix}y_0\\ \vdots\\y_n\end{matrix}\right]$$

简化写成$e=Aa-y$

$E_2=\Vert e\Vert_2^2=e^Te=(Aa-y)^T(Aa-y)$

$=(a^TA^T-y^T)(Aa-y)$

因此$E_2(a)=a^TA^TAa-2a^TA^Ty+Y^Ty$

$\frac{\partial E_2}{\partial a}=0=2A^TAa-2A^Ty$

So, $(A^TA)a=A^Ty$

令$B=A^TA, C=A^Ty$

则可以写成$Ba=C$

<br>



# 2. Orthogonal Polynomials and Least Squares Approximation

### 1. Inner Product

A map $V\times V\rightarrow F$ with 3 axioms

* conjugate symmetry(x和y的内积等于y和x的内积)

$$<x,y>=\overline{<y,x>}$$

* Linearity

$$<ax,y>=a<x,y>,\quad<x+y,z>=<x,z>+<y,z>$$

* Positive-definiteness

$$<x,x>\;\geq 0,\quad <x,x>=0\Rightarrow x=0 $$

内积和范数的关系

Inner product defines a norm of $x$ as $\Vert x\Vert_2=\sqrt{<x,x>}$

扩展到向量和函数

* Discrete

$$\sum_{i} f_ig_i=\left[ \begin{matrix} f_1\;f_2&\cdots f_n\end{matrix}\right] \left[\begin{matrix}g_1\\g_2\\\cdots\\g_n\end{matrix}\right]=(f)^T(g)$$

* Continous

$$\int_a^b f(x)g(x)dx$$

<br>

> $f$ and $g$ are said to be ***orthogonal*** if $(f,g)=0$



### 2. Weight(metrix)

$<f,g>_w$:

* Discrete

$$\begin{aligned}\sum_{i} w_if_ig_i=&\left[\begin{matrix}f_1&f_2&\cdots&f_n\end{matrix}\right]\left[\begin{matrix}w_1&0&\cdots&0\\0&w_2&\cdots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&w_n\end{matrix}\right]\left[\begin{matrix}g_1\\g_2\\\vdots\\g_n\end{matrix}\right]\\=&(f)^T[w](g)\end{aligned}$$

* Continous

$$\int_a^b w(x)f(x)g(x)dx$$

<br>

为什么要有这个东西呢？

考虑我们解方程得到了一个误差

$$\left\Vert\begin{matrix}e_1\\e_2\\\cdots\\e_n\end{matrix}\right\Vert_2$$

假设我们不在乎$x_1$的误差，特别在乎$x_2$的误差

那我们可以写出

$$E=0.1e_1^2+10e_2^2+\cdots$$

相当于每个$e_i$前面乘上了一个权重

$$E=e^T\left[\begin{matrix}0.1&0\\0&10\end{matrix}\right]e=e^Twe$$

<br>

回到原来的问题

* Given $x_1,\cdots,x_m$ and $y_1,\cdots,y_m$. Find a simpler function $P(x)\approx f(x)$ such that $E=\sum_{i=1}^m\vert P(x_i)-y_i\vert^2$ is minimized

* Given a function $f(x)$ defined on $[a,b]$. Find a simpler function $P(x)\approx f(x)$ such that $E=\int_a^b[P(x)-f(x)]^2dx$ is minimized.



#### Linearly Independent Functions

`Definition`{:.warning}

The set of functions $\{\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x) \}$ is said to be ***Linearly independent*** on $[a,b]$ if, whenever

$$a_0\varphi_0(x)+a_1\varphi_1(x)+\cdots+a_n\varphi_n(x)=0\quad for\;all\;x\in [a,b]$$

we have $a_0=a_1=\cdots=a_n=0$. Otherwise the set of functiosn is said to be ***linearly dependent***

<br>

`Theorem`{:.error}

If $\varphi_j(x)$ is a polynomial of degree $j$ for each $j=0,\cdots,n$, then $\{\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x))\}$ is linearly independent on any interval $[a,b]$

<br>

`Theorem`{:.error}

Let $\Pi_n$ be the set of all polynomials of degree at most $n$. If $\{\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x) \}$ is a collection of  linearly independent polynomials in $\Pi_n$, then any polynomials in $\Pi_n$ can be written uniquely as a linear combination of $\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x)$

<br>

`Definition`{:.warning}

For a general linear independent set of functions $\{\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x) \}$, a linear combination of $\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x)$, $P(x)=\sum_{j=0}^n a_j\varphi_j(x)$ is called a **generalized polynomial**



### Approximation

In a space spanned $\{\varphi_i(x)\}$, find the one that is closest to $f$

* The element in the space

$$P(a,x)=\sum_i a_i\phi_i(x)=(\phi(x))^T(a)$$

* The distance(error) between $P(a)$ and $f$

$$E(a)=E(a_0,\cdots,a_n)=\Vert P(a)-f\Vert^2=<\phi(x_i)a-f,\cdots>$$

$=a^T\varphi\varphi^T a-2a^T\varphi f+f^Tf$

$$E=e_i^2=\left\Vert\begin{matrix}e_1\\\cdots\\c_n\end{matrix}\right\Vert_2^2$$

$e_i=(\phi(x_i))^T(a)-f_i$

$$\left[\begin{matrix}e_1\\\cdots\\e_n\end{matrix}\right]=\left[\begin{matrix}\phi(x_1)^T\\\vdots\\ \phi(x_n)^T\end{matrix}\right] \left[\begin{matrix}a_0\\\vdots \\a_n \end{matrix}\right]-\left[\begin{matrix}y_0\\ \vdots\\y_n\end{matrix}\right]$$

简化写成$e=\phi a-f$

$E_2=\Vert e\Vert_2^2=e^Te=(\phi^T a-f)^T(\phi^T a-f)$

$=(a^T\phi-f^T)(\phi^T a-y)$

因此$E(a)=a^T\phi\phi^T a-2a^T\phi f+f^Tf$

* minimize the distance

$0=\frac{\partial E}{\partial a}=2\phi\phi^T a-2\phi f$

<br>

#### Normal equations

$$\begin{aligned}0&=2\phi\phi^T a-2\phi f \\ \phi\phi^Ta&=\phi f\\ (<\phi_i,\phi_j>_w)a&=(<\phi,f>_w)\end{aligned}$$

当$w$是SPD，则$<\phi,f>_w$是SPD matrix

<br>

$\phi_i$代表的是$\phi$矩阵中的一列

<br>

***Example***: Approximate 

| x    | 1    | 2    | 3    | 4    |
| ---- | ---- | ---- | ---- | ---- |
| y    | 4    | 10   | 18   | 26   |

with $y=a_0+a_1x+a_2x^2$ and $w\equiv 1$

Solution:

First construct the orthogonal polynomials $\varphi_0(x),\varphi_1(x),\varphi_2(x)$

$\varphi_0(x)=1,\varphi_1(x)=x,\varphi_2(x)=x^2$

$$y=a_0\varphi_0(x)+a_1\varphi_1(x)+a_2\varphi_2(x)$$

$$\begin{aligned}(\varphi_0,\varphi_0)&=\sum_{i=1}^4 1\cdot 1=4\\ (\varphi_0,\varphi_1)&=\sum_{i=1}^4 1\cdot x_i=10\\(\varphi_0,\varphi_2)&=\sum_{i=1}^41\cdot x_i^2=30 \\(\varphi_1,\varphi_2)&=\sum_{i=1}^4x_i\cdot x_i^2=100\\(\varphi_1,\varphi_1)&=\sum_{i=1}^4x_i^2=30\\(\varphi_2,\varphi_2)&=\sum_{i=1}^4x_i^4=354\\\end{aligned}$$

$(\varphi_0,y)=\sum_{i=1}^41\cdot y_i=58\quad (\varphi_1,y)=182\quad (\varphi_0,y)=622$

<br>

$$\left[\begin{matrix}4&10&30\\10&30&100\\30&100&354\end{matrix}\right]\left[\begin{matrix}a_0\\a_1\\a_2\end{matrix}\right]\left[\begin{matrix}a58\\182\\622\end{matrix}\right]$$

解得$a_0=-\frac{3}{2},a_1=\frac{49}{10},a_2=\frac{1}{2}$

$y=P(x)=\frac{1}{2}x^2+\frac{49}{10}x-\frac{3}{2}$

但是这种做法条件数会变得非常的差

$\Vert B\Vert_\infty=484,\Vert B^{-1}\Vert=\frac{63}{4}\Rightarrow K(B)=7623$

<br>

Example:

<br>

#### Improvement

如果我们可以找到一组基函数$\{\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x) \}$是的$\varphi_i(x)$和$\varphi_j(x)$是正交的。那么normal matrix就是对角矩阵

这样可以直接解出$a$, $a_k=\frac{<\varphi_k,f>}{<\varphi_k,\varphi_k>}$

Construction of the orthogonal polynomials：

<a href="https://www.zhihu.com/question/60689540">施密特正交化</a>

`Theorem`{:.error}

The set of polynomial functions $\{\varphi_0(x),\varphi_1(x),\cdots,\varphi_n(x)\}$ defined in the following way is orthogonal on $[a,b]$ with respect to the weight function $w$.

$\varphi_0(x)=1,\varphi_1(x)=x-B_1$

$\varphi_k(x)=(x-B_k)\varphi_{k-1}(x)-C_k\varphi_{k-2}(x)$

where $B_k=\frac{(x\varphi_{k-1},\varphi_{k-1})}{(\varphi_{k-1},\varphi_{k-1})}, C_k=\frac{(x\varphi_{k-1},\varphi_{k-2})}{(\varphi_{k-2},\varphi_{k-2})}$



