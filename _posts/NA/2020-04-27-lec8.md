---
title: "Approximation Theory"
tags: Numarical_Analysis
key: page-NA8



---

<!--more-->



给定$x_1,\cdots,x_m$, 以及$y_1,\cdots,y_m$, 找一个更简单的函数$P(x)\approx f(x)$

但是

* m通常很大
* $y_i$是实验数据，并不精确，也就是说$y_i\not= f(x_i)$

我们的目标是找到best fit $P(x)$ 使得误差$P(x_i)-y_i$对所有数据点都小

Options:

* minimize $\underset{1\leq i\leq m}{max}\vert P(x_i)-y_i\vert$  minmax problem 最大的误差最小化(对应∞范数)
* minimize $\sum_{i=1}^m\vert P(x_i)-y_i\vert$ absolute deviation 对应L-1范数
* minimize $\sum_{i=1}^m \vert P(x_i)-y_i\vert ^2$ least-squares method 对应L-2范数



# 1. Discrete Least Squares Approximation

Determine the polynomial $P_n(x)=a_0+a_1x+\cdots+a_nx^n$ to approximate a set of data $\{(x_i,y_i)\vert i=1,2,\cdots,m\}$ such that least squares error $E_2=\sum_{i=1}^m[P_n(x_i)-y_i]^2$ is minimized.

Here, $n<<m$

事实上，$E_2$是$a_0,a_1,\cdots,a_n$的函数，即

$$E_2(a_0,a_1,\cdots,a_n)=\sum_{i=1}^m[a_0+a_1x_i+\cdots+a_nx_i^n-y_i]^2$$

要最小化$E_2$, 则要使$\frac{\partial E_2}{\partial a_k}=0,k=0,\cdots,n$

$$\begin{aligned}0&=\frac{\partial E_2}{\partial a_k}\\&=2\sum_{i=1}^m[P_n(x_i)-y_i]\cdot \frac{\partial P_n(x_i)}{\partial a_k}\\&=2\sum_{i=1}^m[\sum_{j=0}^na_jx_i^j-y_i]x_i^k\\&=2(\sum_{j=0}^na_j(\sum_{i=1}^mx_i^{j+k})-\sum_{i=1}^my_ix_i^k )\end{aligned}$$

令$b_k=\sum_{i=1}^m x_i^k, c_k=\sum_{i=1}^my_ix_i^k$

对所有$a_k$求导，写成矩阵形式

$$\left[\begin{matrix}b_{0+0}&\cdots&b_{0+n}\\\vdots&\vdots&\vdots\\ b_{n+0}&\cdots&b_{n+n} \end{matrix}\right]\left[\begin{matrix}a_0\\\vdots \\a_n\end{matrix}\right]=\left[\begin{matrix}c_0\\ \vdots\\ c_n\end{matrix}\right]$$



另一种推导方式

$$\begin{aligned}P(x)=&\alpha_0x^0+\alpha_1x^1+\cdots+\alpha_nx^n\\=&(x^0, x^1,\cdots,x^n)\left[\begin{matrix}\alpha_0\\\alpha_1\\\vdots\\\alpha_n\end{matrix}\right]  =(\phi(x))(a)\end{aligned}$$



$$E=e_i^2=\Vert\left[\begin{matrix}e_1\\\cdots\\c_n\end{matrix}\right]\Vert_2^2$$

$e_i=(\phi(x_i))(a)-y_i$

$$\left[\begin{matrix}e_1\\\cdots\\e_n\end{matrix}\right]=\left[\begin{matrix}\phi(x_1)\\\vdots\\ \phi(x_n)\end{matrix}\right] \left[\begin{matrix}a_0\\\vdots \\a_n \end{matrix}\right]-\left[\begin{matrix}y_0\\ \vdots\\y_n\end{matrix}\right]$$

简化写成$e=Aa-y$

$E_2=\Vert e\Vert_2^2=e^Te=(Aa-y)^T(Aa-y)$

$=(a^TA^T-y^T)(Aa-y)$

因此$E_2(a)=a^TA^TAa-2a^TA^Ty+Y^Ty$

$\frac{\partial E_2}{\partial a}=0=2A^TAa-2A^Ty$

So, $(A^TA)a=A^Ty$

令$B=A^TA, C=A^Ty$

则可以写成$Ba=C$











