---
title: "Iterative Techniques in Matrix Algebra"
tags: Numarical_Analysis
key: page-NA5






---

<!--more-->

# 1. Norms or Vectors 

## A. Vector Norms

`Definition `{:.success}

A vector norm on $\mathbb{R}^n$ is a function, $\Vert\cdot\Vert$ from $\mathbb{R}^n$ into $\mathbb{R}$ 

1. $\Vert \vec{x}\Vert\geq 0$ for all $\vec{x}\in\mathbb{R}^n$
2. $\Vert \vec{x}\Vert=0$  if and only if $\vec{x}=0$
3. $\Vert\alpha \vec{x}\Vert=\vert\alpha\vert\Vert \vec{x}\Vert$ for all $\alpha\in\mathbb{R}$ and $\vec{x}\in\mathbb{R}^n$
4. $\Vert \vec{x}+\vec{y}\Vert\leq \Vert \vec{x}\Vert+\Vert \vec{y}\Vert$ for all $\vec{x},\vec{y}\in\mathbb{R}^n$

<br>

Some popular used norms:

$\Vert \vec{x}\Vert_1=\sum_{i=1}^n\vert x_i\vert$

$\Vert \vec{x}\Vert_2=\sqrt{\sum_{i=1}^n\vert x_i\vert^2}$

$\Vert \vec{x}\Vert_p=(\sum_{i=1}^n\vert x_i\vert^p)^{\frac{1}{p}}$

$\Vert \vec{x}\Vert_\infty=\underset{1\leq i\leq n}{max}\vert x_i\vert$

$\underset{p\rightarrow\infty}{lim}\Vert\vec{x}\Vert_p=\Vert \vec{x}\Vert_\infty$

<img src="../../../assets/images/image-20200323143045024.png" alt="image-20200323143045024" style="zoom:50%;" />

<img src="/Users/jones/Library/Application Support/typora-user-images/image-20200323143057221.png" alt="image-20200323143057221" style="zoom:50%;" />

向量的收敛

`Definition`{:.success}

A sequence $\{x^{(k)}\}_{k=1}^\infty$ of vectors in $\mathbb{R}^n$  is said to converge to $x$ with respect to the norm  $\Vert\cdot\Vert$  if, given any ε > 0, there exists an integer N(ε) such that

$$\Vert x^{(k)}-x\Vert\leq\varepsilon\quad for\;all\;k\geq N(\varepsilon)$$

<br>

`Theorem`{:.error} 

The sequence of vectors $\{x^{(k)}\}$ converges to $x$ in $\mathbb{R}^n$ with respect to the $l_\infty$ norm if and only if $\underset{k\rightarrow\infty}{lim} x_i^{(k)}=x_i$, for each $i=1,2\cdots,n$

Proof:

Suppose $\{x^{(k)}\}$ converges to $x$ with respect to the $l_\infty$ norm. Given any $\varepsilon>0$, there exists an integer $N(\varepsilon)$ such that for all $k\geq N(\varepsilon)$

$$\underset{i=1,2,\cdots,n}{max}\vert x_i^{(k)}-x_i\vert=\Vert x^{(k)}-x\Vert_\infty<\varepsilon$$

<br>

`Definition`{:.success}

If there exist positive constants $C_1$ and $C_2$ such that $C_1\Vert x\Vert_B\leq\Vert x\Vert_A\leq\ C_2\Vert x\Vert_N$, then $\Vert\cdot\Vert_A$ and $\Vert\cdot\Vert_B$ are said to be equivalent.

<br>

`Theorem`{:.error}

All the vector norms on $\mathbb{R}^n $ are equivalent.

<br>

`Theorem`{:.error}

For each $x\in\mathbb{R}^n$,

$$\Vert x\Vert_\infty\leq\Vert x\Vert_2\leq \sqrt{n}\Vert x\Vert_\infty$$

Proof: Let $x_j$ be a coordinate of x such that $\Vert x\Vert_\infty=\underset{1\leq i\leq n}{max}\vert x_i\vert=\vert x_j\vert$. Then

$$\Vert x\Vert_\infty^2=\vert x_j\vert ^2=x_j^2\leq \sum_{i=1}^nx_i^2=\Vert x\Vert_2^2$$

and $\Vert x\Vert_\infty\leq \Vert x\Vert_2$

So

$$\Vert x\Vert_2^2=\sum_{i=1}^n x_i^2\leq nx_j^2=n\Vert x\Vert_\infty^2$$

and $\Vert x\Vert_2\leq\sqrt{n}\Vert x\Vert_\infty$

<br>

<img src="../../../assets/images/image-20200329124640498.png" alt="image-20200329124640498" style="zoom:50%;" />





## B. Matrix Norms

`Definition`{:.success}

A matrix norm on the set of all $n\times n$ matrices is a real-valued function, $\Vert\cdot\Vert$, defined on this set , satisfying for all $n\times n$ matrices A and B and all $\alpha\in C$

<br>

1. $\Vert A\Vert\geq 0$; $\Vert A\Vert=0\Longleftrightarrow A=\mathbf{0}$
2. $\Vert \alpha A\Vert=\vert\alpha\vert\cdot\Vert A\Vert$
3. $\Vert A+B\Vert\leq \Vert A\Vert +\Vert B\Vert$
4. $\Vert AB\Vert\leq \Vert A\Vert\cdot \Vert B\Vert$

<br>

Some popularly used norms:

* Frebenius Norm

$$\Vert A\Vert_F=\sqrt{\sum_{i=1}^n\sum_{j=1}^n\vert a_{ij}\vert^2}$$

* Natual Norm:

$\Vert A\Vert_p=\underset{\vec{x}\not=0}{max}\frac{\Vert A\vec{x}\Vert_p}{\Vert \vec{x}\Vert_p}=\underset{\Vert x\Vert_p=1}{max}\Vert A\vec{x}\Vert_p$  associated with the vector norm $\Vert\cdot\Vert_p$

$\|A\|$是最大放大系数

<br>

$$\begin{aligned}\Vert A\Vert_\infty&=\underset{i\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert\\ \Vert A\Vert_1&=\underset{1\leq j\leq n}{max}\sum_{i=1}^n\vert a_{ij}\vert\\ \Vert A\Vert_2&=\sqrt{\lambda_{max}(A^TA)}\end{aligned}$$

<br>

Proof $\Vert A\Vert_\infty=\underset{i\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$

1. First we show that $\Vert A\Vert_\infty\leq \underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$

Let x be an n-dimensional vector with $1=\Vert x\Vert_\infty=\underset{1\leq i\leq n}{max}\vert x_i\vert$. Since Ax is also an n-dimensional vector

$$\Vert Ax\Vert_\infty=\underset{1\leq i\leq n}{max}\vert (Ax)_i\vert=\underset{1\leq i\leq n}{max}\vert \sum_{j=1}^n a_{ij}x_j\vert\leq \underset{1\leq i\leq n}{max}\sum_{j=1}^n \vert a_{ij}\vert \underset{1\leq j\leq n}{max}\vert x_j\vert$$

But $\underset{1\leq i\leq n}{max}\vert x_j\vert=\Vert x\Vert_\infty=1$, so

$$\Vert Ax\Vert_\infty\leq \underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

2. Then we show show that $\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert\leq \Vert A\Vert_\infty$

Let p be an intege with

$$\sum_{j=1}^n\vert a_{pj}\vert=\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

即第p行是系数和最大的一行

and x be the vector with components

$$x_j=\begin{aligned}1&,\;if\;a_{pj}\geq 0\\-1&,\; if\; a_{pj}<0\end{aligned}$$

Then $\Vert x\Vert_\infty=1$ and $a_{pj}x_j=\vert a_{pj}\vert$, for all $j=1,2,\cdots,n$, so

$$\Vert Ax\Vert_\infty=\underset{1\leq i\leq n}{max}\vert \sum_{j=1}^n a_{ij}x_j\vert\geq \vert \sum_{j=1}^n a_{pj}x_j\vert =\vert \sum_{j=1}^n\vert a_{pj}\vert\;\vert=\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

This result implies that 

$$\Vert A\Vert_\infty=\underset{\Vert x\Vert_\infty=1}{max}>\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

So, we have $\Vert A\Vert_\infty=\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$

<br>

<img src="../../../assets/images/image-20200329130542847.png" alt="image-20200329130542847" style="zoom:50%;" />





# 2. Eigenvalues and Eigenvectors

Spectral Radius

`Definition`{:.success}

The spectral radius $\rho(A)$ of a matrix A is defined by $\rho(A)=max\vert\lambda\vert$ where $\lambda$ is an eigenvalue of A.

<img src="../../../assets/images/image-20200329193423639.png" alt="image-20200329193423639" style="zoom:50%;" />

<br>

`Theorem`{:.error}

If A is an $n\times n$ matrix, then $\rho(A)\leq\Vert A\Vert$ for any natural norm $\Vert\cdot\Vert$

Proof: For any eigenvalue $\lambda$ of A with eigenvector $\vec{x}$ and $\Vert\vec{x}\Vert=1$

$$\vert\lambda\vert\cdot\Vert\vec{x}\Vert=\Vert\lambda\vec{x}\Vert=\Vert A\vec{x}\Vert\leq \Vert A\Vert\cdot\Vert\vec{x}\Vert$$

<br>

`Definition`{:.warning}

We call an $n\times n$ matrix A convergent if for all $i,j=1,2,\cdots,n$, we have $\underset{k\rightarrow\infty}{lim}(A^k)_{ij}=0$

<br>

# 3. Iterative Techniques

## A. Jacobi Iterative Method

$$\begin{aligned}a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=b_1\\a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n=b_2\\ \cdots\quad \cdots\quad \cdots\quad \cdots\\a_{n1}x_1+a_{n2}x_2+\cdots+a_{nn}x_n=b_n\end{aligned}$$

If $a_{ii}\not= 0$

Then

$$\begin{aligned}x_1&=\frac{1}{a_{11}}(-a_{12}x_2-\cdots-a_{1n}x_n+b_1)\\ x_2&=\frac{1}{a_{22}}(-a_{21}x_1-\cdots-a_{2n}x_n+b_1)\\ &\cdots\quad \cdots\quad\cdots\quad\cdots\\ x_n&=\frac{1}{a_{nn}}(-a_{n1}x_1-\cdots-a_{nn-1}x_{n-1}+b_n)\end{aligned}$$

<br>

That is 

$$x_i=\underset{j=1,j\not= i}{\overset{n}\sum}(-\frac{a_{ij}x_j}{a_{ii}})+\frac{b_i}{a_{ii}},\quad for\; i=1,2,\cdots,n$$

For each $k\geq 1$, generate the components $x_i^{(k)}$ of $x^{(k)}$ from the components of $x^{(k-1)}$ by

$$x_i^{(k)}=\frac{1}{a_{ii}}[\underset{j=1,j\not=i}{\overset{n} \sum}(-a_{ij}x_j^{(k-1)})+b_i],\;for\; i=1,2,\cdots,n$$

<img src="../../../assets/images/image-20200329143426997.png" alt="image-20200329143426997" style="zoom:50%;" />

$(D-L-U)x=b$, then transformed into

$$Dx=(L+U)x+b$$

and, if $D^{-1}$ exists, that is, if $a_{ii}\not= 0$  for each i, then

$$x=D^{-1}(L+U)x+D^{-1}b$$

This results in the matrix form of the Jacobi iterative technique:

$$x^{(k)}=D^{-1}(L+U)x^{(k-1)}+D^{-1}b,\quad k=1,2,\cdots$$

Introducting the notation $T_j=D^{-1}(L+U)$ and $c_j=D^{-1}b$ gives the Jacobi technique the form

$$x^{(k)}=T_jx^{(k-1)}+c_j$$



`Jacobi iterative algorithm`{:.warning}

To solve $Ax = b$ given an initial approximation $x^{(0)}$ :

**INPUT** the number of equations and unknowns n; the entries $a_{ij}$ , 1 ≤ i, j ≤ n of the matrix A; the entries $b_i$ , 1 ≤ i ≤ n of b; the entries $XO_i$ , 1 ≤ i ≤ n of $XO = x^{(0)}$ ; tolerance TOL; maximum number of iterations N.

**OUTPUT** the approximate solution $x_1,\cdots,x_n$ or a message that the number of iterations was exceeded.

**Step 1** Set k = 1.

**Step 2** While($k\leq N$) do Steps 3-6.

&emsp; **Step 3** For $i=1,\cdots,n$

&emsp;&emsp;&emsp;set $x_i=\frac{1}{a_{ii}}[-\underset{j=1,j\not=i}{\overset{n}\sum}(a_{ij}XO_j)+b_i]$

&emsp;**Step 4** If $\Vert x-XO\Vert<TOL$ then OUTPUT($x_1,\cdots,x_n$);

&emsp;&emsp;&emsp;&emsp;STOP. (The procedure was successful.)

&emsp;**Step 5** Set k = k + 1.

&emsp;**Step 6** For $i=1,\cdots,n$ set $XO_i=x_i$

&emsp;**Step 7** OUTPUT('Maximum number of iterations execeeded');

<br>

## B. Gauss-Seidel Iterative Method

$$\begin{aligned}x_2^{(k)}&=\frac{1}{a_{22}}(-a_{21}x_1^{(k)}-a_{23}x_3^{(k-1)}-a_{24}x_4^{(k-1)}-\cdots-a_{2n}x_n^{(k-1)}+b_2)\\ x_3^{(k)}&=\frac{1}{a_{33}}(-a_{31}x_1^{(k)}-a_{32}x_2^{(k)}-a_{34}x_4^{(k-1)}-\cdots-a_{3n}x_n^{(k-1)}+b_3)\\&\cdots\quad \cdots\quad\cdots\quad\cdots\\ x_n^{(k)}&=\frac{1}{a_{nn}}(-a_{n1}x_1^{(k)}-a_{n2}x_2^{(k)}-a_{n3}x_3^{(k)}-\cdots-a_{nn-1}x_n^{(k-1)}+b_n)\end{aligned}$$

可以看到$x_1^{(k)}$被计算出来之后，就被用去计算$x_2^{(k)}$, $x_2^{(k)}$被计算出来之后，马上就被用去计算$x_3^{(k)}$。这样会比用$x_1^{(k-1)}$去估计$x_2^{(k)}$收敛地更快

In matrix form:

$$\vec{x}^{(k)}=D^{-1}(L\vec{x}^{(k)}+U\vec{x}^{(k-1)})+D^{-1}\vec{b}$$

$$(D-L)\vec{x}^{(k)}=U\vec{x}^{(k-1)}+\vec{b}$$

$$\vec{x}^{(k)}=(D-L)^{-1}U\vec{x}^{(k-1)}+(D-L)^{-1}\vec{b}$$

<br>

Note:  

Neither of the methods are always convergent.

And more, there are cases in which Jacobi method fails while Gauss-Seidel is convergent, and vice-versa.

## C. Convergence of Iterative Methods

$$\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}$$

`Theorem`{:.error}

The following statements are equivalent:

1. A is a convergent matrix
2. $\underset{n\rightarrow\infty}{lim}\Vert A^n\Vert=0$ for some natural norm
3. $\underset{n\rightarrow\infty}{lim}\Vert A^n\Vert=0$ for some natural norms
4. $\rho(A)<1$
5. $\underset{n\rightarrow\infty}{lim}A^n\vec{x}=\vec{0}$ for every $\vec{x}$

<br>

$$\vec{e}^{(k)}=\vec{x}^{(k)}-\vec{x}^*=(T\vec{x}^{(k-1)}+\vec{c})-(T\vec{x}^*+\vec{c})=T(\vec{x}^{(k-1)}-\vec{x}^*)=T\vec{e}^{(k-1)}$$

$\Rightarrow \vec{e}^{(k)}=T^k\vec{e}^{(0)}$

$\Vert\vec{e}^{(k)}\Vert\leq \Vert T\Vert\Vert \vec{e}^{(k-1)}\Vert\leq\cdots\leq\Vert T\Vert^k\Vert\vec{e}^{(0)}\Vert$

* Sufficient condition: $\Vert T\Vert<1\Longrightarrow \Vert T\Vert^k\rightarrow 0\;as\;k\rightarrow\infty$

比如考虑

$$\left[\begin{matrix}e_1^{(k)}\\e_2^{(k)}\end{matrix}\right]=\left[\begin{matrix}T_{11}&0\\0&T_{22}\end{matrix}\right]\left[\begin{matrix}e_1^{(k-1)}\\e_2^{(k-1)}\end{matrix}\right]$$

如果$e_2^{(k-1)}=0$ 那么$T_{22}=1000$依然不影响$e_2^{(k)}$, 但是$\Vert T\Vert>1$

* Necessarg condition: $e^{(0)}\rightarrow\vec{0}$ as $k\rightarrow\infty\Longrightarrow T^k\rightarrow0$

<br>

`Theorem`{:.error}

For any $\vec{x^{(0)}}\in\R^n$, the sequence $\{\vec{x}^{(k)}\}_{k=0}^\infty$ defined by $\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}$ for each $k\geq 1$, converges to the unique solution of $\vec{x}=T\vec{x}+\vec{c}$ if and only if $\rho(T)<1$. (这是一个不动点迭代)

Proof:

$\rho(T)<1$代表了什么？说明这个矩阵一直都在缩小所乘的向量

Lemma: If the spectral radius satisﬁes $\rho(T) < 1$, then $(I − T) ^{−1}$ exists, and

$$(I-T)^{-1}=\sum_{j=0}^\infty T^j$$

> Proof the lemma:
>
> Because $Tx=\lambda x$ is true precisely when $(1-T)x=(1-\lambda)x$, we have $\lambda$ as an eigenvalue of T precisely when $1-\lambda$ is an eigenvalue of $1-T$.
>
> But $\vert\lambda\vert\leq\rho(T)<1$, so $\lambda=1$ is not an eigenvalue of T, and 0 cannot be an eigenvalue of $1-T$. Hence, $(1-T)^{-1}$ exists.
>
> <br>
>
> Let $S_m=I+T+T^2+\cdots+T^m$. Then
>
> $(I-T)S_m=(I+T+T^2+\cdots+T^m)-(T+T^2+\cdots+T^{m+1})=I-T^{m+1}$
>
> and, since T is convergent
>
> $$\underset{m\rightarrow\infty}{lim}(I-T)S_m=\underset{m\rightarrow}{lim}(I-T^{m+1})=I$$
>
> Thus, $(I-T)^{-1}=\underset{m\rightarrow\infty}{lim}S_m=I+T+T^2+\cdots=\sum_{j=0}^\infty T^j$

1. Assume $\rho(T)<1 $

$x^{(k)}=Tx^{(k-1)}+c=T(Tx^{(k-2)}+c)+c=T^2x^{(k-2)}+(T+I)c$

$=\cdots=T^kx^{(0)}+(T^{k-1}+\cdots+T+I)c$

对于$T^kx^{(0)}$, 首先因为$\rho(T)<1$，因此T在不断缩小$x^{(0)}$向量，最终这项会趋向于0

对于$(T^{k-1}+\cdots+T+I)c$

$$\rho(T)<1\Longrightarrow (I-T)^{-1}=\sum_{j=0}^\infty T^j$$

$$\underset{k\rightarrow\infty}{lim}x^{(k)}=\underset{k\rightarrow\infty}{lim}T^kx^{(0)}+\underset{k\rightarrow\infty}{lim}(I+T+T^2+\cdots+T^{k-1})c=(I-T)^{-1}c$$

So, $x=Tx+c$

2. Let $z$ be an arbitrary vector, and x be the unique solution to $x = Tx + c$. Deﬁne $x^{(0)} = x − z$, and, for k ≥ 1, $x^{(k)} = Tx ^{(k−1)} + c$. Then {$x^{(k)}$ } converges to x. Also,

$x-x^{(k)}=(Tx+c)-(Tx^{(k-1)}+c)=T(x-x^{(k-1)})$

so

$x-x^{(k)}=T(x-x^{(k-1)})=T^2(x-x^{(k-2)})=\cdots=T^k(x-x^{(0)})=T^kz\rightarrow 0$

Because $z\in\R^n$ was arbitrary, so $\rho(T)<1$

<br>

`Theorem`{:.error}

If $\Vert T\Vert < 1$ for any natural matrix norm and c is a given vector, then the sequence $\{x^{(k)}\}_{k=0}^\infty$ defined by $x^{(k)}=Tx^{(k-1)}+c$ converges, for any $x^{(0)}\in\R^n$, to a vector $x\in\R^n$, with $x=Tx+c$, and the following error bounds hold

1. $\Vert x-x^{(k)}\Vert\leq \Vert T\Vert^k\Vert x^{(0)}-x\Vert$
2. $\Vert x-x^{(k)}\Vert\leq \frac{\Vert T\Vert^k}{1-\Vert T\Vert}\Vert x^{(1)}-x^{(0)}\Vert$

familar?

<img src="../../../assets/images/image-20200329215559501.png" alt="image-20200329215559501" style="zoom:33%;" />



`Theorem`{:.error}

If A is strictly diagonally dominant, then for any choice of $x^{(0)}$ , both the Jacobi and Gauss-Seidel methods give sequences $ \{x^{(k)}\}_{k=0}^\infty$ that converge to the unique solution of Ax = b.

# 4. Relaxation Method

`Definition`{:.success}

Suppose $\tilde{x} \in \R^n$ is an approximation to the solution of the linear system deﬁned by $Ax = b$. The residual vector for $\tilde{x}$ with respect to this system is $r = b − A\tilde{x}$.

<br>

Examine Gauss-Seidel method from another angle

$x_i^{(k)}=\frac{1}{a_{ii}}[b_i-\sum_{j=1}^{i-1}a_{ij}x_i^{(k)}-\sum_{j=i+1}^na_{ij}x_j^{(k-1)}]$

$=x_i^{(k-1)}+\frac{r_i^{(k)}}{a_{ii}}$ where $r_i^{(k)}=b_i-\sum_{j<i}a_{ij}x_j^{(k)}-\sum_{j\geq i}a_{ij}x_j^{(k-1)}$

<br>

Let $x_i^{(k)}=x_i^{(k-1)}+\omega\frac{r_i^{(k)}}{a_{ii}}$. For a certain choice of positive $\omega$, we can reduce the norm of the residual vector and obtain faster convergence. Such methods are called ***Relaxation Methods.***

* $0<\omega<1$: Under-Relaxation method
* $\omega=1$ Gauss-Seidel
* $\omega>1$: Successive Over-Relaxation methods

<br>

In matrix form:

$x_i^{(k)}=x_i^{(k-1)}+\omega\frac{r_i^{(k)}}{a_{ii}}=(1-\omega)x_i^{(k-1)}+\frac{\omega}{a_{ii}}[-\sum_{j<i}a_{ij}x_j^{(k)}-\sum_{j>i}a_{ij}x_j^{(k-1)}+b_i]$

$\Rightarrow x^{(k)}=(1-\omega)x^{(k-1)}+\omega D^{-1}[Lx^{(k)}+Ux^{(k-1)}+b]$

$\Rightarrow x^{(k)}=(D-\omega L)^{-1}[(1-\omega)D+\omega U]x^{((k-1))}+(D-\omega L)^{-1}\omega b$

$\Rightarrow x^{(k)}=T_{\omega} x^{(k-1)}+c_{\omega}b$

<br>

`Theorem(Kahan)`{:.error}

If $a_{ii}\not=0$, for each $i=1,2,\cdots,n$, then $\rho(T_\omega)\geq\vert\omega-1\vert$. This implies that the SOR method can converge only if $0<\omega<2$

<br>

`Theorem(Ostrowski-Reich)`{:.error}

If A is **positive definite** and $0<\omega<2$, the SOR method converges for any choice of initial approximation.

这个定理太好了吧。构造symmetric positive definite $J^TJ$

<br>

`Theorem`{:.error}

If A is positive deﬁnite and tridiagonal, then $\rho(T_g ) = [\rho(T_j )]^2 < 1$, and the optimal choice of ω for the SOR method is

$$\omega=\frac{2}{1+\sqrt{1-[\rho(T_j)]^2}}$$

With this choice of $\omega$, we have $\rho(T_{\omega})=\omega-1$

<br>

Example:

Given $A=\left[\begin{matrix}2&1\\1&2\end{matrix}\right]$, $\vec{b}=\left[\begin{matrix}1\\2\end{matrix}\right]$ and an iterative method $\vec{x}^{(k)}=\vec{x}^{(k-1)}+\omega(A\vec{x}^{(k-1)}-\vec{b})$, Then

1. For waht values of $\omega$ that the method will converge?
2. For what values of $\omega$ that the method will have the fastest convergence?

Solution:

$\vec{x}^{(k)}=(I+\omega A)\vec{x}^{(k-1)}-\omega\vec{b}$

Consider the eigenvalues of $T=I+\omega A$







# 4. Error Bounds and Iterative Refinement

How will the errors of A and $\vec{b}$ affect the solution $\vec{x}$ of $A\vec{x}=\vec{b}$

Assume that A is accurate, and $\vec{b}$ has the error $\delta\vec{b}$. Then the solution with error can be written as $\vec{x}+\delta\vec{x}$. That is:

$$A(\vec{x}+\delta\vec{x})=\vec{b}+\delta\vec{b}$$

$$\frac{\Delta \vec{x}\Vert}{\Vert\vec{x}\Vert}\leq \Vert A\Vert\cdot\Vert A^{-1}\Vert\frac{\Vert\delta\vec{b}\Vert}{\Vert\vec{b}\Vert}$$













