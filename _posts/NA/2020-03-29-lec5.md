---
title: "Iterative Techniques in Matrix Algebra"
tags: Numarical_Analysis
key: page-NA5
---

<!--more-->

# 1. Norms or Vectors 

## A. Vector Norms

`Definition `{:.success}

A vector norm on $\mathbb{R}^n$ is a function, $\Vert\cdot\Vert$ from $\mathbb{R}^n$ into $\mathbb{R}$ 

1. $\Vert \vec{x}\Vert\geq 0$ for all $\vec{x}\in\mathbb{R}^n$
2. $\Vert \vec{x}\Vert=0$  if and only if $\vec{x}=0$
3. $\Vert\alpha \vec{x}\Vert=\vert\alpha\vert\Vert \vec{x}\Vert$ for all $\alpha\in\mathbb{R}$ and $\vec{x}\in\mathbb{R}^n$
4. $\Vert \vec{x}+\vec{y}\Vert\leq \Vert \vec{x}\Vert+\Vert \vec{y}\Vert$ for all $\vec{x},\vec{y}\in\mathbb{R}^n$

<br>

Some popular used norms:

$\Vert \vec{x}\Vert_1=\sum_{i=1}^n\vert x_i\vert$

$\Vert \vec{x}\Vert_2=\sqrt{\sum_{i=1}^n\vert x_i\vert^2}$

$\Vert \vec{x}\Vert_p=(\sum_{i=1}^n\vert x_i\vert^p)^{\frac{1}{p}}$

$\Vert \vec{x}\Vert_\infty=\underset{1\leq i\leq n}{max}\vert x_i\vert$

$\underset{p\rightarrow\infty}{lim}\Vert\vec{x}\Vert_p=\Vert \vec{x}\Vert_\infty$

<img src="../../../assets/images/image-20200323143045024.png" alt="image-20200323143045024" style="zoom:50%;" />

<img src="../../../assets/images/image-20200323143057221.png" alt="image-20200323143057221" style="zoom:50%;" />

向量的收敛

`Definition`{:.success}

A sequence $\{x^{(k)}\}_{k=1}^\infty$ of vectors in $\mathbb{R}^n$  is said to converge to $x$ with respect to the norm  $\Vert\cdot\Vert$  if, given any ε > 0, there exists an integer N(ε) such that

$$\Vert x^{(k)}-x\Vert\leq\varepsilon\quad for\;all\;k\geq N(\varepsilon)$$

<br>

`Theorem`{:.error} 

The sequence of vectors $\{x^{(k)}\}$ converges to $x$ in $\mathbb{R}^n$ with respect to the $l_\infty$ norm if and only if $\underset{k\rightarrow\infty}{lim} x_i^{(k)}=x_i$, for each $i=1,2\cdots,n$

Proof:

Suppose $\{x^{(k)}\}$ converges to $x$ with respect to the $l_\infty$ norm. Given any $\varepsilon>0$, there exists an integer $N(\varepsilon)$ such that for all $k\geq N(\varepsilon)$

$$\underset{i=1,2,\cdots,n}{max}\vert x_i^{(k)}-x_i\vert=\Vert x^{(k)}-x\Vert_\infty<\varepsilon$$

<br>

`Definition`{:.success}

If there exist positive constants $C_1$ and $C_2$ such that $C_1\Vert x\Vert_B\leq\Vert x\Vert_A\leq\ C_2\Vert x\Vert_N$, then $\Vert\cdot\Vert_A$ and $\Vert\cdot\Vert_B$ are said to be equivalent.

<br>

`Theorem`{:.error}

All the vector norms on $\mathbb{R}^n $ are equivalent.

<br>

`Theorem`{:.error}

For each $x\in\mathbb{R}^n$,

$$\Vert x\Vert_\infty\leq\Vert x\Vert_2\leq \sqrt{n}\Vert x\Vert_\infty$$

Proof: Let $x_j$ be a coordinate of x such that $\Vert x\Vert_\infty=\underset{1\leq i\leq n}{max}\vert x_i\vert=\vert x_j\vert$. Then

$$\Vert x\Vert_\infty^2=\vert x_j\vert ^2=x_j^2\leq \sum_{i=1}^nx_i^2=\Vert x\Vert_2^2$$

and $\Vert x\Vert_\infty\leq \Vert x\Vert_2$

So

$$\Vert x\Vert_2^2=\sum_{i=1}^n x_i^2\leq nx_j^2=n\Vert x\Vert_\infty^2$$

and $\Vert x\Vert_2\leq\sqrt{n}\Vert x\Vert_\infty$

<br>

<img src="../../../assets/images/image-20200329124640498.png" alt="image-20200329124640498" style="zoom:50%;" />





## B. Matrix Norms

`Definition`{:.success}

A matrix norm on the set of all $n\times n$ matrices is a real-valued function, $\Vert\cdot\Vert$, defined on this set , satisfying for all $n\times n$ matrices A and B and all $\alpha\in C$

<br>

1. $\Vert A\Vert\geq 0$; $\Vert A\Vert=0\Longleftrightarrow A=\mathbf{0}$
2. $\Vert \alpha A\Vert=\vert\alpha\vert\cdot\Vert A\Vert$
3. $\Vert A+B\Vert\leq \Vert A\Vert +\Vert B\Vert$
4. $\Vert AB\Vert\leq \Vert A\Vert\cdot \Vert B\Vert$

<br>

Some popularly used norms:

* Frebenius Norm

$$\Vert A\Vert_F=\sqrt{\sum_{i=1}^n\sum_{j=1}^n\vert a_{ij}\vert^2}$$

* Natual Norm:

$\Vert A\Vert_p=\underset{\vec{x}\not=0}{max}\frac{\Vert A\vec{x}\Vert_p}{\Vert \vec{x}\Vert_p}=\underset{\Vert x\Vert_p=1}{max}\Vert A\vec{x}\Vert_p$  associated with the vector norm $\Vert\cdot\Vert_p$

$\|A\|$是最大放大系数

<br>

$$\begin{aligned}\Vert A\Vert_\infty&=\underset{i\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert\\ \Vert A\Vert_1&=\underset{1\leq j\leq n}{max}\sum_{i=1}^n\vert a_{ij}\vert\\ \Vert A\Vert_2&=\sqrt{\lambda_{max}(A^TA)}\end{aligned}$$

<br>

Proof $\Vert A\Vert_\infty=\underset{i\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$

1. First we show that $\Vert A\Vert_\infty\leq \underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$

Let x be an n-dimensional vector with $1=\Vert x\Vert_\infty=\underset{1\leq i\leq n}{max}\vert x_i\vert$. Since Ax is also an n-dimensional vector

$$\Vert Ax\Vert_\infty=\underset{1\leq i\leq n}{max}\vert (Ax)_i\vert=\underset{1\leq i\leq n}{max}\vert \sum_{j=1}^n a_{ij}x_j\vert\leq \underset{1\leq i\leq n}{max}\sum_{j=1}^n \vert a_{ij}\vert \underset{1\leq j\leq n}{max}\vert x_j\vert$$

But $\underset{1\leq i\leq n}{max}\vert x_j\vert=\Vert x\Vert_\infty=1$, so

$$\Vert Ax\Vert_\infty\leq \underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

2. Then we show show that $\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert\leq \Vert A\Vert_\infty$

Let p be an intege with

$$\sum_{j=1}^n\vert a_{pj}\vert=\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

即第p行是系数和最大的一行

and x be the vector with components

$$x_j=\begin{aligned}1&,\;if\;a_{pj}\geq 0\\-1&,\; if\; a_{pj}<0\end{aligned}$$

Then $\Vert x\Vert_\infty=1$ and $a_{pj}x_j=\vert a_{pj}\vert$, for all $j=1,2,\cdots,n$, so

$$\Vert Ax\Vert_\infty=\underset{1\leq i\leq n}{max}\vert \sum_{j=1}^n a_{ij}x_j\vert\geq \vert \sum_{j=1}^n a_{pj}x_j\vert =\vert \sum_{j=1}^n\vert a_{pj}\vert\;\vert=\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

This result implies that 

$$\Vert A\Vert_\infty=\underset{\Vert x\Vert_\infty=1}{max}>\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$$

So, we have $\Vert A\Vert_\infty=\underset{1\leq i\leq n}{max}\sum_{j=1}^n\vert a_{ij}\vert$

<br>

<img src="../../../assets/images/image-20200329130542847.png" alt="image-20200329130542847" style="zoom:50%;" />





# 2. Eigenvalues and Eigenvectors

Spectral Radius

`Definition`{:.success}

The spectral radius $\rho(A)$ of a matrix A is defined by $\rho(A)=max\vert\lambda\vert$ where $\lambda$ is an eigenvalue of A.

<img src="../../../assets/images/image-20200329193423639.png" alt="image-20200329193423639" style="zoom:50%;" />

<br>

`Theorem`{:.error}

If A is an $n\times n$ matrix, then $\rho(A)\leq\Vert A\Vert$ for any natural norm $\Vert\cdot\Vert$

Proof: For any eigenvalue $\lambda$ of A with eigenvector $\vec{x}$ and $\Vert\vec{x}\Vert=1$

$$\vert\lambda\vert\cdot\Vert\vec{x}\Vert=\Vert\lambda\vec{x}\Vert=\Vert A\vec{x}\Vert\leq \Vert A\Vert\cdot\Vert\vec{x}\Vert$$

<br>

`Definition`{:.warning}

We call an $n\times n$ matrix A convergent if for all $i,j=1,2,\cdots,n$, we have $\underset{k\rightarrow\infty}{lim}(A^k)_{ij}=0$

<br>

# 3. Iterative Techniques

## A. Jacobi Iterative Method

$$\begin{aligned}a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=b_1\\a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n=b_2\\ \cdots\quad \cdots\quad \cdots\quad \cdots\\a_{n1}x_1+a_{n2}x_2+\cdots+a_{nn}x_n=b_n\end{aligned}$$

If $a_{ii}\not= 0$

Then

$$\begin{aligned}x_1&=\frac{1}{a_{11}}(-a_{12}x_2-\cdots-a_{1n}x_n+b_1)\\ x_2&=\frac{1}{a_{22}}(-a_{21}x_1-\cdots-a_{2n}x_n+b_1)\\ &\cdots\quad \cdots\quad\cdots\quad\cdots\\ x_n&=\frac{1}{a_{nn}}(-a_{n1}x_1-\cdots-a_{nn-1}x_{n-1}+b_n)\end{aligned}$$

<br>

That is 

$$x_i=\underset{j=1,j\not= i}{\overset{n}\sum}(-\frac{a_{ij}x_j}{a_{ii}})+\frac{b_i}{a_{ii}},\quad for\; i=1,2,\cdots,n$$

For each $k\geq 1$, generate the components $x_i^{(k)}$ of $x^{(k)}$ from the components of $x^{(k-1)}$ by

$$x_i^{(k)}=\frac{1}{a_{ii}}[\underset{j=1,j\not=i}{\overset{n} \sum}(-a_{ij}x_j^{(k-1)})+b_i],\;for\; i=1,2,\cdots,n$$

<img src="../../../assets/images/image-20200329143426997.png" alt="image-20200329143426997" style="zoom:50%;" />

$(D-L-U)x=b$, then transformed into

$$Dx=(L+U)x+b$$

and, if $D^{-1}$ exists, that is, if $a_{ii}\not= 0$  for each i, then

$$x=D^{-1}(L+U)x+D^{-1}b$$

This results in the matrix form of the Jacobi iterative technique:

$$x^{(k)}=D^{-1}(L+U)x^{(k-1)}+D^{-1}b,\quad k=1,2,\cdots$$

Introducting the notation $T_j=D^{-1}(L+U)$ and $c_j=D^{-1}b$ gives the Jacobi technique the form

$$x^{(k)}=T_jx^{(k-1)}+c_j$$



`Jacobi iterative algorithm`{:.warning}

To solve $Ax = b$ given an initial approximation $x^{(0)}$ :

**INPUT** the number of equations and unknowns n; the entries $a_{ij}$ , 1 ≤ i, j ≤ n of the matrix A; the entries $b_i$ , 1 ≤ i ≤ n of b; the entries $XO_i$ , 1 ≤ i ≤ n of $XO = x^{(0)}$ ; tolerance TOL; maximum number of iterations N.

**OUTPUT** the approximate solution $x_1,\cdots,x_n$ or a message that the number of iterations was exceeded.

**Step 1** Set k = 1.

**Step 2** While($k\leq N$) do Steps 3-6.

&emsp; **Step 3** For $i=1,\cdots,n$

&emsp;&emsp;&emsp;set $x_i=\frac{1}{a_{ii}}[-\underset{j=1,j\not=i}{\overset{n}\sum}(a_{ij}XO_j)+b_i]$

&emsp;**Step 4** If $\Vert x-XO\Vert<TOL$ then OUTPUT($x_1,\cdots,x_n$);

&emsp;&emsp;&emsp;&emsp;STOP. (The procedure was successful.)

&emsp;**Step 5** Set k = k + 1.

&emsp;**Step 6** For $i=1,\cdots,n$ set $XO_i=x_i$

&emsp;**Step 7** OUTPUT('Maximum number of iterations execeeded');

<br>

## B. Gauss-Seidel Iterative Method

$$\begin{aligned}x_2^{(k)}&=\frac{1}{a_{22}}(-a_{21}x_1^{(k)}-a_{23}x_3^{(k-1)}-a_{24}x_4^{(k-1)}-\cdots-a_{2n}x_n^{(k-1)}+b_2)\\ x_3^{(k)}&=\frac{1}{a_{33}}(-a_{31}x_1^{(k)}-a_{32}x_2^{(k)}-a_{34}x_4^{(k-1)}-\cdots-a_{3n}x_n^{(k-1)}+b_3)\\&\cdots\quad \cdots\quad\cdots\quad\cdots\\ x_n^{(k)}&=\frac{1}{a_{nn}}(-a_{n1}x_1^{(k)}-a_{n2}x_2^{(k)}-a_{n3}x_3^{(k)}-\cdots-a_{nn-1}x_n^{(k-1)}+b_n)\end{aligned}$$

可以看到$x_1^{(k)}$被计算出来之后，就被用去计算$x_2^{(k)}$, $x_2^{(k)}$被计算出来之后，马上就被用去计算$x_3^{(k)}$。这样会比用$x_1^{(k-1)}$去估计$x_2^{(k)}$收敛地更快

In matrix form:

$$\vec{x}^{(k)}=D^{-1}(L\vec{x}^{(k)}+U\vec{x}^{(k-1)})+D^{-1}\vec{b}$$

$$(D-L)\vec{x}^{(k)}=U\vec{x}^{(k-1)}+\vec{b}$$

$$\vec{x}^{(k)}=(D-L)^{-1}U\vec{x}^{(k-1)}+(D-L)^{-1}\vec{b}$$

<br>

Note:  

Neither of the methods are always convergent.

And more, there are cases in which Jacobi method fails while Gauss-Seidel is convergent, and vice-versa.

## C. Convergence of Iterative Methods

$$\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}$$

`Theorem`{:.error}

The following statements are equivalent:

1. A is a convergent matrix
2. $\underset{n\rightarrow\infty}{lim}\Vert A^n\Vert=0$ for some natural norm
3. $\underset{n\rightarrow\infty}{lim}\Vert A^n\Vert=0$ for some natural norms
4. $\rho(A)<1$
5. $\underset{n\rightarrow\infty}{lim}A^n\vec{x}=\vec{0}$ for every $\vec{x}$

<br>

$$\vec{e}^{(k)}=\vec{x}^{(k)}-\vec{x}^*=(T\vec{x}^{(k-1)}+\vec{c})-(T\vec{x}^*+\vec{c})=T(\vec{x}^{(k-1)}-\vec{x}^*)=T\vec{e}^{(k-1)}$$

$\Rightarrow \vec{e}^{(k)}=T^k\vec{e}^{(0)}$

$\Vert\vec{e}^{(k)}\Vert\leq \Vert T\Vert\Vert \vec{e}^{(k-1)}\Vert\leq\cdots\leq\Vert T\Vert^k\Vert\vec{e}^{(0)}\Vert$

* Sufficient condition: $\Vert T\Vert<1\Longrightarrow \Vert T\Vert^k\rightarrow 0\;as\;k\rightarrow\infty$

比如考虑

$$\left[\begin{matrix}e_1^{(k)}\\e_2^{(k)}\end{matrix}\right]=\left[\begin{matrix}T_{11}&0\\0&T_{22}\end{matrix}\right]\left[\begin{matrix}e_1^{(k-1)}\\e_2^{(k-1)}\end{matrix}\right]$$

如果$e_2^{(k-1)}=0$ 那么$T_{22}=1000$依然不影响$e_2^{(k)}$, 但是$\Vert T\Vert>1$

* Necessarg condition: $e^{(0)}\rightarrow\vec{0}$ as $k\rightarrow\infty\Longrightarrow T^k\rightarrow0$

<br>

`Theorem`{:.error}

For any $\vec{x^{(0)}}\in\mathbb{R}^n$, the sequence $\{\vec{x}^{(k)}\}_{k=0}^\infty$ defined by $\vec{x}^{(k)}=T\vec{x}^{(k-1)}+\vec{c}$ for each $k\geq 1$, converges to the unique solution of $\vec{x}=T\vec{x}+\vec{c}$ if and only if $\rho(T)<1$. (这是一个不动点迭代)

Proof:

$\rho(T)<1$代表了什么？说明这个矩阵一直都在缩小所乘的向量

Lemma: If the spectral radius satisﬁes $\rho(T) < 1$, then $(I − T) ^{−1}$ exists, and

$$(I-T)^{-1}=\sum_{j=0}^\infty T^j$$

> Proof the lemma:
>
> Because $Tx=\lambda x$ is true precisely when $(1-T)x=(1-\lambda)x$, we have $\lambda$ as an eigenvalue of T precisely when $1-\lambda$ is an eigenvalue of $1-T$.
>
> But $\vert\lambda\vert\leq\rho(T)<1$, so $\lambda=1$ is not an eigenvalue of T, and 0 cannot be an eigenvalue of $1-T$. Hence, $(1-T)^{-1}$ exists.
>
> <br>
>
> Let $S_m=I+T+T^2+\cdots+T^m$. Then
>
> $(I-T)S_m=(I+T+T^2+\cdots+T^m)-(T+T^2+\cdots+T^{m+1})=I-T^{m+1}$
>
> and, since T is convergent
>
> $$\underset{m\rightarrow\infty}{lim}(I-T)S_m=\underset{m\rightarrow}{lim}(I-T^{m+1})=I$$
>
> Thus, $(I-T)^{-1}=\underset{m\rightarrow\infty}{lim}S_m=I+T+T^2+\cdots=\sum_{j=0}^\infty T^j$

1. Assume $\rho(T)<1 $

$x^{(k)}=Tx^{(k-1)}+c=T(Tx^{(k-2)}+c)+c=T^2x^{(k-2)}+(T+I)c$

$=\cdots=T^kx^{(0)}+(T^{k-1}+\cdots+T+I)c$

对于$T^kx^{(0)}$, 首先因为$\rho(T)<1$，因此T在不断缩小$x^{(0)}$向量，最终这项会趋向于0

对于$(T^{k-1}+\cdots+T+I)c$

$$\rho(T)<1\Longrightarrow (I-T)^{-1}=\sum_{j=0}^\infty T^j$$

$$\underset{k\rightarrow\infty}{lim}x^{(k)}=\underset{k\rightarrow\infty}{lim}T^kx^{(0)}+\underset{k\rightarrow\infty}{lim}(I+T+T^2+\cdots+T^{k-1})c=(I-T)^{-1}c$$

So, $x=Tx+c$

2. Let $z$ be an arbitrary vector, and x be the unique solution to $x = Tx + c$. Deﬁne $x^{(0)} = x − z$, and, for k ≥ 1, $x^{(k)} = Tx ^{(k−1)} + c$. Then {$x^{(k)}$ } converges to x. Also,

$x-x^{(k)}=(Tx+c)-(Tx^{(k-1)}+c)=T(x-x^{(k-1)})$

so

$x-x^{(k)}=T(x-x^{(k-1)})=T^2(x-x^{(k-2)})=\cdots=T^k(x-x^{(0)})=T^kz\rightarrow 0$

Because $z\in\mathbb{R}^n$ was arbitrary, so $\rho(T)<1$

<br>

`Theorem`{:.error}

If $\Vert T\Vert < 1$ for any natural matrix norm and c is a given vector, then the sequence $\{x^{(k)}\}_{k=0}^\infty$ defined by $x^{(k)}=Tx^{(k-1)}+c$ converges, for any $x^{(0)}\in\\mathbb{R}^n$, to a vector $x\in\mathbb{R}^n$, with $x=Tx+c$, and the following error bounds hold

1. $\Vert x-x^{(k)}\Vert\leq \Vert T\Vert^k\Vert x^{(0)}-x\Vert$
2. $\Vert x-x^{(k)}\Vert\leq \frac{\Vert T\Vert^k}{1-\Vert T\Vert}\Vert x^{(1)}-x^{(0)}\Vert$

familar?

<img src="../../../assets/images/image-20200329215559501.png" alt="image-20200329215559501" style="zoom:33%;" />



`Theorem`{:.error}

If A is strictly diagonally dominant, then for any choice of $x^{(0)}$ , both the Jacobi and Gauss-Seidel methods give sequences $ \{x^{(k)}\}_{k=0}^\infty$ that converge to the unique solution of Ax = b.

# 4. Relaxation Method

`Definition`{:.success}

Suppose $\tilde{x} \in \mathbb{R}^n$ is an approximation to the solution of the linear system deﬁned by $Ax = b$. The residual vector for $\tilde{x}$ with respect to this system is $r = b − A\tilde{x}$.

<br>

Examine Gauss-Seidel method from another angle

$x_i^{(k)}=\frac{1}{a_{ii}}[b_i-\sum_{j=1}^{i-1}a_{ij}x_i^{(k)}-\sum_{j=i+1}^na_{ij}x_j^{(k-1)}]$

$=x_i^{(k-1)}+\frac{r_i^{(k)}}{a_{ii}}$ where $r_i^{(k)}=b_i-\sum_{j<i}a_{ij}x_j^{(k)}-\sum_{j\geq i}a_{ij}x_j^{(k-1)}$

<br>

Let $x_i^{(k)}=x_i^{(k-1)}+\omega\frac{r_i^{(k)}}{a_{ii}}$. For a certain choice of positive $\omega$, we can reduce the norm of the residual vector and obtain faster convergence. Such methods are called ***Relaxation Methods.***

* $0<\omega<1$: Under-Relaxation method
* $\omega=1$ Gauss-Seidel
* $\omega>1$: Successive Over-Relaxation methods

<br>

In matrix form:

$x_i^{(k)}=x_i^{(k-1)}+\omega\frac{r_i^{(k)}}{a_{ii}}=(1-\omega)x_i^{(k-1)}+\frac{\omega}{a_{ii}}[-\sum_{j<i}a_{ij}x_j^{(k)}-\sum_{j>i}a_{ij}x_j^{(k-1)}+b_i]$

$\Rightarrow x^{(k)}=(1-\omega)x^{(k-1)}+\omega D^{-1}[Lx^{(k)}+Ux^{(k-1)}+b]$

$\Rightarrow x^{(k)}=(D-\omega L)^{-1}[(1-\omega)D+\omega U]x^{((k-1))}+(D-\omega L)^{-1}\omega b$

$\Rightarrow x^{(k)}=T_{\omega} x^{(k-1)}+c_{\omega}b$

<br>

`Theorem(Kahan)`{:.error}

If $a_{ii}\not=0$, for each $i=1,2,\cdots,n$, then $\rho(T_\omega)\geq\vert\omega-1\vert$. This implies that the SOR method can converge only if $0<\omega<2$

<br>

`Theorem(Ostrowski-Reich)`{:.error}

If A is **positive definite** and $0<\omega<2$, the SOR method converges for any choice of initial approximation.

这个定理太好了吧。构造symmetric positive definite $J^TJ$

<br>

`Theorem`{:.error}

If A is positive deﬁnite and tridiagonal, then $\rho(T_g ) = [\rho(T_j )]^2 < 1$, and the optimal choice of ω for the SOR method is

$$\omega=\frac{2}{1+\sqrt{1-[\rho(T_j)]^2}}$$

With this choice of $\omega$, we have $\rho(T_{\omega})=\omega-1$

<br>

Example:

Given 

$$A=\left[\begin{matrix}2&1\\1&2\end{matrix}\right], \vec{b}=\left[\begin{matrix}1\\2\end{matrix}\right]$$

 and an iterative method $\vec{x}^{(k)}=\vec{x}^{(k-1)}+\omega(A\vec{x}^{(k-1)}-\vec{b})$, Then

1. For waht values of $\omega$ that the method will converge?
2. For what values of $\omega$ that the method will have the fastest convergence?

Solution:

$\vec{x}^{(k)}=(I+\omega A)\vec{x}^{(k-1)}-\omega\vec{b}$

Consider the eigenvalues of $T=I+\omega A$







# 5. Error Bounds and Iterative Refinement

接下来我们考虑A和$\vec{b}$的误差会对$A\vec{x}=\vec{b}$的解$\vec{x}$造成怎样的影响

不妨假设A不存在误差，$\vec{b}$的误差为$\delta\vec{b}$, 解可以写成$\vec{x}+\delta\vec{x}$

$$A(\vec{x}+\delta\vec{x})=\vec{b}+\delta\vec{b}$$

那么$\delta\vec{x}$和$\delta{b}$有什么样的关系呢？下面做一些简单推导:

$A\vec{x}=\vec{b}, A\delta\vec{x}=\vec{b}$

A是可逆矩阵，那么

$\delta\vec{x}=A^{-1}\delta\vec{b}$

两边取范数，得到

$$\Vert\delta\vec{x}\Vert=\Vert A^{-1}\delta\vec{b}\Vert\leq \Vert A^{-1}\Vert\Vert\delta\vec{b}\Vert$$

同时$Ax=b\rightarrow \Vert A\Vert \Vert x\Vert\geq \Vert b\Vert$

因此$\frac{\Vert\delta x\Vert}{\Vert x\Vert}\leq \Vert A\Vert A^{-1}\Vert\frac{\Vert\delta b\Vert}{\Vert b\Vert}$

<br>

如果b是精确的A存在误差$\delta A$的话

先提出一个定理: If a matrix B sitisfies $\Vert B\Vert <1$ for some natural norm, then $I\pm B$ is nonsingular; and $\Vert(I\pm B)^{-1}\Vert \leq\frac{1}{1-\Vert B\Vert}$

$(A+\delta A)(x+\delta x)=b$

因为$(A+\delta A)=A(I+A^{-1}\delta A)$, 因为$\delta A$非常小，我们可以假设$\Vert A^{-1}\delta A\Vert << 1$, 那么根据定理$I+A^{-1}\delta A$是可逆矩阵

$(A+\delta x)\delta x+\delta Ax=0$

因此$\delta x=-(I+A^{-1}\delta A)^{-1}A^{-1}\delta Ax$

$\frac{\delta x}{x}=-(I+A^{-1}\delta A)^{-1}A^{-1}\delta A$

取范数后

$\frac{\Vert \delta x\Vert }{\Vert x\Vert}\leq \frac{\Vert A^{-1}\Vert\Vert\delta A\Vert}{1-\Vert A^{-1}\Vert\Vert\delta A\Vert}=\frac{\Vert A\Vert\Vert A^{-1}\Vert\frac{\Vert\delta A\Vert}{\Vert A\Vert}}{1-\Vert A^{-1}\Vert \frac{\Vert\delta A\Vert}{\Vert A\Vert}}$

<br>

我们发现$\Vert A\Vert\Vert A^{-1}\Vert$是误差放大的一个关键因素

因此引入一个新的概念`condition number`{:.error}

定义一个矩阵A的条件数为

$K(A)_p=\Vert A^{-1}\Vert_p\Vert A\Vert_p$

condition number越大，越难得到正确的解

接下来我们看一下condition number的性质

* $K(A)_p\geq 1$

$K(A)_p=\Vert A^{-1}\Vert_p\Vert A\Vert_p\geq \Vert A^{-1}A\Vert_p=\Vert I\Vert_p$

因此$K(A)_p\geq 1$

* $K(\alpha A)=K(A)$

$K(\alpha A)_p=\Vert \alpha^{-1}A^{-1}\Vert_p\Vert \alpha A\Vert_p=K(A)_p$

*  $K(A)_2=\frac{max\vert\lambda\vert}{min\vert\lambda\vert}$

$\Vert A\Vert_2=max\frac{\Vert Ax\Vert_2}{\Vert x\Vert_2}$

$\Vert A^{-1}\Vert_2=\frac{1}{min\frac{\Vert Ax\Vert_2}{\Vert x\Vert_2}}$

因此 $K(A)_2=\frac{max\vert\lambda\vert}{min\vert\lambda\vert}$

* $K(A)_2=1$ if A is orthogonal $(A^{-1}=A^t)$
* $K(RA)_2=K(AR)_2=K(A)_2$ for all orthogonal matrix R



Example:

(a) $$\left[\begin{matrix}1&1\\ 1&1.0001\end{matrix}\right]x=\left[\begin{matrix}2\\ 2.0001\end{matrix}\right]$$

(b) $$\left[\begin{matrix}1&1\\ 1&1.0001\end{matrix}\right]x=\left[\begin{matrix}2\\ 2\end{matrix}\right]$$

对于矩阵A, $\lambda_1=2.00005,\lambda_2=0.00005$

所以条件数$K(A)_2>40000$

然而$\frac{\Vert \delta b\Vert}{\Vert b\Vert}=\frac{0.0001}{2}<0.01\%$

因此条件数会对解的精确程度产生巨大的影响

<br>

### Iterative Refinement

当condition number比较大，但是不是特别大的时候，可以进行改进

For a solution $x_1$(got via LU method), calculate the remaining vector(residue vector)

$$r_1=b-Ax_1$$

For the $r$, more accuracy is necessary, because the $b$ and $Ax$ are relatively large. This means while calculating the $Ax_1$, more accuracy is required.

$Ad_1=r_1$

$x_2=x_1+d_1$

Now we have a sequence $\{x_i\}$,

$Ax_{n+1}=A(x_n+d_n)=Ax_n+Ad_n=Ax_n+r_n=b$

<br>

This seems to be a perfect solution. However, because the $r_n$ itself can have rounding error, and the condition number $\Vert A\Vert\Vert A^{-1}\Vert$ is a large number, this method maybe unstable,too.

<br>

Example:

$$\left[\begin{matrix} 1.0303 & 0.99030\\0.99030&0.95285\end{matrix}\right]\left[\begin{matrix} x_1\\x_2\end{matrix}\right]=\left[\begin{matrix}2.4944\\2.3988\end{matrix}\right]$$

The specific solution $x^*=(1.2240,1.2454)^T$

$$A=\left[\begin{matrix}1&0\\0.9118&1\end{matrix}\right] \left[\begin{matrix}1.0303&0.99030\\0&0.0009\end{matrix}\right]=LU$$

$x_1=(1.2560,1.2121)^T$

Start the iteration

| i    | $x_i$               | $r_i$                               | $d_i$                                |
| ---- | ------------------- | ----------------------------------- | ------------------------------------ |
| 1    | $(1.2560,1.2121)^T$ | $(5.7*10^{-7},3.3715*10^{-5})^T$    | $(-0.03220,0.033502)^T$              |
| 2    | $(1.2238,1.2456)^T$ | $(1.18*10^{-6},9*10^{-7})^T$        | $(2.285*10^{-4},-2.365*10^{-4})$     |
| 3    | $(1.2240,1.2454)^T$ | $(-0.682*10^{-5},-0.659*10^{-5})^T$ | $(0.2717*10^{-4},-0.3515*10^{-4})^T$ |







# 6. deep thoughts on Matrix

接下来深入理解一下矩阵

我们可以认为矩阵的两个作用分别时旋转和放缩，将矩阵进行SVD分解

$A=USV^T$, $U,V$是正交矩阵(旋转矩阵)， $U^{-1}=U^T, V^{-1}=V^T,S$是对角阵

$Ax=USV^Tx$

<img src="../../../assets/images/image-20200406175924151.png" alt="image-20200406175924151" style="zoom:50%;" />

<br>

可逆矩阵

<img src="../../../assets/images/image-20200406161452735.png" alt="image-20200406161452735" style="zoom:50%;" />

条件数

<img src="../../../assets/images/image-20200406164331004.png" alt="image-20200406164331004" style="zoom:50%;" />

考虑几个例子:

$$A=\left[\begin{matrix}10^3&0&0&\cdots\\0&1&0&\cdots\\\vdots&\vdots&\vdots& \vdots\\0&\cdots&0&10^{-3} \end{matrix}\right]$$

$det(A)=1, K_2(A)=\frac{10^3}{10^{-3}}=10^{6}$

所以这个矩阵对应的解是很不稳定的

$$A^{-1}=\left[\begin{matrix}10^{-3}&0&0&\cdots\\0&1&0&\cdots\\\vdots&\vdots&\vdots& \vdots\\0&\cdots&0&10^3 \end{matrix}\right]$$

$$x=\left[\begin{matrix}x_1\\\vdots\\ x_n\end{matrix}\right],b=\left[\begin{matrix}b_1\\\vdots\\ b_n\end{matrix}\right]$$

$Ax=b\Rightarrow \begin{aligned}&10^3x_1=b_1\\&10^{-3}x_n=b_n\end{aligned}$

也就是在$x_1$方向放大1000倍，一点小的误差也会被放大1000倍; 在$x_n$方向被缩小到原来的$\frac{1}{1000}$, 也就是说A的作用是各向异性的。

<img src="../../../assets/images/image-20200406171213328.png" alt="image-20200406171213328" style="zoom:50%;" />

各向异性的映射会对应上图，将一个圆压成了一个椭圆，极端情况下甚至发生降维，映射成一条线。在被压缩的方向上的一个微小扰动做逆映射就会产生巨大的误差。

而condition number恰恰反映了这种各项异性，如果condition number越大，说明各向异性越明显。

<br>

再考虑正交矩阵的condition number, 前面提到$K_2(A)=1$, 这是因为正交矩阵是一个旋转矩阵，只会对向量做一个旋转，不会放大缩小，因此保持各项同性。

$$\Vert Ax\Vert_2^2=(Ax)^T(Ax)=x^TA^TAx=x^T(A^TA)x=x^TIx=\Vert x\Vert_2^2$$

正交矩阵保持向量长度不变

<br>

再来理解$K(RA)_2=K(AR)_2=K(A)_2$ for all orthogonal matrix R

因为R是正交矩阵，只会对椭圆进行旋转，不会改变长轴短轴的长度。

<img src="../../../assets/images/image-20200406173609360.png" alt="image-20200406173609360" style="zoom:50%;" />

