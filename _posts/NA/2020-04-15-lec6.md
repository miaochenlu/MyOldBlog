---
title: "Approximating Eigenvalues"
tags: Numarical_Analysis
key: page-NA6

---

<!--more-->

计算一个矩阵的dominant特征值及其对应的特征向量

# 1. The power method

The Original Method

假设: A是一个$n\times n$的矩阵，他的特征值满足$\vert\lambda_1\vert>\vert\lambda_2\vert\geq \cdots\vert\lambda_n\vert\geq 0$. 这些特征值对应了n个线性无关的特征向量$\vec{v_1},\cdots,\vec{v_n}$, 这n个向量可以作为线性空间的一组基。

想法是:

从$\vec{x}^{(0)}\not=\vec{0}$开始，并且 $(\vec{x}^{(0)},\vec{v_1})\not=0$

$\vec{x}^{(0)}=\sum_{j=1}^n\beta_j\vec{v}_j,\;\beta_1\not=0$

$\vec{x}^{(1)}=A\vec{x}^{(0)}=\sum_{j=1}^n\beta_j\lambda_j\vec{v}_j$ (因为$Ax=\lambda x$)

$\vec{x}^{(2)}=A\vec{x}^{(1)}=\sum_{j=1}^n\beta_j\lambda_j^2\vec{v}_j$

$\cdots$

$$\vec{x}^{(k)}=A\vec{x}^{(k-1)}=\sum_{j=1}^n\beta_j\lambda_j^k{\vec{v}_j}=\lambda_1^k\sum_{j=1}^n\beta_j(\frac{\lambda_j}{\lambda_1})^k{\vec{v}_j}$$

当k大到一定程度后，$(\frac{\lambda_j}{\lambda_1})^k\rightarrow 0$

因此

$\vec{x}^{(k)}\approx \lambda_1^k\beta_1\vec{v_1},\;\;\vec{x}^{(k-1)}=\lambda^{k-1}\beta_1\vec{v_1}\;\;\Rightarrow \frac{(\vec{x}^{(k)})_i}{(\vec{x}^{(k-1)})_i}\approx\lambda_1$

这个过程相当于不做用矩阵A去作用来扩大各项异性，如果不知道这块知识可以去看"Iterative Techniques in Matrix Algebra" chapter 6

<img src="../../../assets/images/image-20200407180611463.png" alt="image-20200407180611463" style="zoom:50%;" />

<br>

但是，上述方法存在一个问题，比如$\lambda_1=2,k=100$ , $2^{100}$直接溢出, 因此每次迭代后要去做normalization

### Normalization

每一步都保证$\Vert \vec{x}\Vert_\infty=1$来维持稳定性。最直接的方法是向量中的所有元素都除以最大值

Let $\Vert\vec{x}^{(k)}\Vert_\infty=\vert x_{p_k}^{(k)}\vert$  ($x_{p_k}^{(k)}$对应最大的那个元素). Then

$$\vec{u}^{(k-1)}=\frac{\vec{x}^{(k-1)}}{\vec{x}^{(k-1)}_{p_{k-1}}}\;   and  \;\vec{x}^{(k)}=A\vec{u}^{(k-1)}$$

$$\vec{u}^{(k)}=\frac{\vec{x}^{(k)}}{\vec{x}^{(k)}_{p_{k-1}}}\rightarrow \vec{v}_1\;   and \;\lambda_1\approx \frac{x_i^{(k)}}{u_i^{(k-1)}}=x^{(k)}_{p_{k-1}}$$



### Algorithm

To approximate the dominant eigenvalue and an associated eigenvector of the n × n matrix A given a nonzero vector x:

**INPUT** dimension n; matrixA;vector x; tolerance TOL; maximum number of iterations N. 

**OUTPUT** approximate eigenvalue μ; approximate eigenvector x (with $\Vert x\Vert_\infty=1$) or a message that the maximum number of iterations was exceeded.

**Step 1** Set k = 1.

**Step 2** Find the smallest integer p with $1 ≤ p ≤ n$ and $\vert x_p\vert = \Vert x\Vert_\infty$ . 

**Step 3** Set $x = \frac{x}{x_p}$ .

**Step 4** While ($k ≤ N$) do Steps 5–11.

&emsp;**Step 5** Set $ y = Ax$.

&emsp;**Step 6** Set $μ = y_p $.

&emsp;**Step 7** Find the smallest integer p with $1 ≤ p ≤ n$ and $\vert y_p\vert = \Vert y\Vert_\infty$ .

&emsp;**Step 8** If $y_p = 0$ then OUTPUT ('Eigenvector', $x$);

&emsp;&emsp;&emsp;&emsp;&emsp;OUTPUT ('A has the eigenvalue 0, select a new vector x and restart');

&emsp;&emsp;&emsp;&emsp;&emsp;STOP.

&emsp;**Step 9** Set $ERR = \Vert x − \frac{y}{y_p}\Vert_\infty$;

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$x = \frac{y}{y_p}$.

&emsp;**Step 10** If $ERR < TOL$ then OUTPUT (μ, x);

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(The procedure was successful.)  STOP.

&emsp;**Step 11** Set $k = k + 1$.

&emsp;**Step 12** OUTPUT ('The maximum number of iterations exceeded'); 

&emsp;&emsp;&emsp;&emsp;(The procedure was unsuccessful.) STOP.



<br>

Note:

* 如果最大特征根是重根: $\lambda_1=\lambda_2=\cdots=\lambda_r$

$$\vec{x}^{(k)}=\lambda_1^k[\sum_{j=1}^r\beta_j\vec{v}_j+\sum_{j=r+1}^n\beta_j(\frac{\lambda_j}{\lambda_1})^k\vec{v}_j]\approx \lambda_1^k(\sum_{j=1}^r\beta_j\vec{v}_j)$$

还是可以计算特征值，但是计算特征向量会有问题

* 这个方法在$\lambda_1=-\lambda_2$的时候不会收敛
* 我们一开始做了$\vec{x}^{(0)}=\sum_{j=1}^n\beta_j\vec{v}_j,\;\beta_1\not=0$的假设，但是$\beta_1$不一定等于0，因此可以多选几个初始向量。当然如果想要计算第二大特征值的时候，可以让$\beta_1=0$
* 这个方法的收敛速度可以用Aitken's $\Delta^2$ procedure加速



# 2. Rate of Convergence

$\vec{x}^{(k)}=A\vec{x}^{(k-1)}=\lambda_1^k\sum_{j=1}^n\beta_j(\frac{\lambda_j}{\lambda_1})^k\vec{v}_j$

Goal: Make $\vert\frac{\lambda_2}{\lambda_1}\vert$ as small as possible.因为这样收敛得更快。

Assume $\lambda_1>\lambda\geq \cdots\geq \lambda_n$, and $\vert \lambda_2\vert >\vert\lambda_n\vert$

那岂不是直接让$\lambda_2$移到0的位置就行了。hhh并不，因为这样的移动可能会使$\vert \lambda_n\vert>\vert\lambda_1\vert$。所以同时要保证$\vert \lambda_n\vert<\vert\lambda_1\vert$

<img src="../../../assets/images/image-20200412152021368.png" alt="image-20200412152021368" style="zoom:50%;" />

这里需要用到矩阵多项式的知识

普通多项式$P(x)=a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0x^0$

矩阵多项式(假设A是实对称矩阵):

 $P(A)=a_nA^n+a_{n-1}A^{n-1}+\cdots+a_0I$

$=a_n(u\Lambda u^{-1})^n+a_{n-1}(u\Lambda u^{-1})^{n-1}+\cdots$

$=a_n(u\Lambda^n u^{-1})+\cdots$

$$=u\left[\begin{matrix}P(\lambda_1)&0&\cdots \\ 0& P(\lambda_2)&\cdots\\\cdots\end{matrix}\right]u^{-1}$$

成为了一个$\lambda$的多项式

$\lambda(P(A))$ 关联于$P(\lambda_A)$

<br>

令$B=A-pI$, 那么$\vert\lambda I-A\vert = \vert\lambda I-(B+pI)\vert=\vert (\lambda-p)I-B\vert$

$\Rightarrow \lambda_A-p=\lambda_B$

因为$\frac{\vert\lambda_2-p\vert}{\vert\lambda_1-p\vert}<\frac{\vert\lambda_2\vert}{\vert\lambda_1\vert}$, 所以计算B的特征值会比计算A的特征值收敛的更快



# 3. Inverse Power Method

如果A的特征值$\vert\lambda_1\vert\geq \vert\lambda_2\vert\geq\cdots>\vert\lambda_n\vert$, 那么$A^{-1}$有特征值$\vert\frac{1}{\lambda_1}\vert>\vert\frac{1}{\lambda_{n-1}}\vert\geq\cdots\geq\vert\frac{1}{\lambda_1}\vert$, 并且他们对应相同的特征向量

The dominant eigenvalue of $A^{-1}$ is the eigenvalue of A with the smallest magnitude.

<br>

Q: How must we compute $\vec{x}^{(k+1)}=A^{-1}\vec{x}^{(k)}$ in every step?

A: Solve a linear system $A\vec{x}^{(k+1)}=\vec{x}^{(k)}$ with A factorized.

如果我们知道A的一个特征值$\lambda_i$接近于一个数值p, 那么我们怎么去更好的去寻找p的近似值

Idea: 对于任何$j\not=i,\vert\lambda_i-p\vert <<\vert\lambda_j-p\vert$. $(A-pI)^{-1}$存在的话，就可以用inverse power method来更快地找到$(A-pI)^{-1}$的dominant eigenvalue $\frac{1}{(\lambda_i-p)}$

