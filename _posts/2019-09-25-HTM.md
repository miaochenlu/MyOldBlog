---
title: "HTM fighting!"

tags: HTM
key: page-HTM
---








# 2019.09.25

Read DHTM and have a general idea!

<br/>

## <u>general idea</u>

NVM, persistent, demand crash recoverability$\Rightarrow$ACID Transactions$\Rightarrow$Software based signigicant performance overheads.  

Propose DHTM.  

Techniques:  

* leverages a ***commercial HTM*** to provide ***atomic visibility*** and extends it with hardware support for ***redo logging*** to provide ***atomic durability***
* extend the supported ***transaction size*** (from being L1-limited to LLC- limited

Result:DHTM outperforms the state-of- the-art by an average of 21% to 25% 

<br/>

## <u>Dive into details</u>

### <u>1 What is the problem?</u>

#### <u>1.1 New things came out, brought opportunities</u>

NVM's emergence provides a high-bandwidth and low-latency alternative for durability.   

##### New challenge

It's good, but could it be better? What if a crash happens? Could it still guarantee durability?  

<br/>

#### <u>1.2 Looking for methods around us</u>

When referring crash consitent, we can think about transactions in database that provide ACID guarantees.    

But these database implementations perform badly when applied to in-memory settings.  

This lead to a question:  

> **How fast can we enforce ACID in the presence of fast persistent memory? **

<br/>

### <u>2 Others' methods and their drawbacks.</u>

#### <u>2.1 first class</u>

Techniques 

> Support atomic durability via software logging by employing flushing and ordering instructions.

Drawbacks 

> significant performance cost

*why????????*

<br/>

#### <u>2.2 second class</u>

Techniques

> * Employ hardware support for atomic durability
>
> **OR**
>
> * leverage hardware support for ordering to guarantee atomic durability

<br/>

#### <u>2.3 third class</u>

techniques

> Use HTM to support atomic visibility

Drawbacks

> * Current HTM only support small transactions
>
> L1 limitation significantly limit usability and efficiency for ACID transactions
>
> * HTM systems only provide ACI guarantees

<br/>

#### <u>2.4 Fourth class</u>

techniques

> supports ACID by integrating HTM with hardware support for durability

Drawback

> PTM (the only proposal in this class) not only introduces significant changes to the cache hierarchy, but also continues to suffer from the L1 limitation.

<img src="http://miaochenlu.github.io/picture/image-20190925202905366.png" alt="image-20190925202905366" style="zoom:80%;" />

<br/>

<br/>

### <u>3 DHTM  sketch</u>

#### *<u>3.1 Goal</u>*

> * Primary: design an HTM that can support ACID transactions efficiently
> * extend the supported transaction size by supporting overflows from the L1 cache to the last level cache (LLC) without adding significant complexity to the coherence pro- tocol or the LLC

#### *<u>3.2 techniques</u>*

Propose DHTM

> * Integrate commercial HTM with hardware support for ***redo logging***. We also propose a mechanism for **coalescing log entries** to reduce the required memory bandwidth.
>
> atomic visibility---->HTM   
>
> 
>
> Atomic durability---> architectural support for transparently and efficiently writing redo log entries to a durable transaction log maintained in persistent memory. 
>
> 
>
> * Leveraging the same logging infrastructure for also supporting ***L1 overflows to extend supported transaction size***. We accomplish this with only ***minor changes*** to coherence protocol.



<br/>

### <u>4 Related Background</u>

#### *<u>A Hardware Transactional Memory</u>*

##### <u>A.1 functionalities</u>

> * Buffering the speculative state
> * tracking read sets and detecting conflicts
> * tracking write sets and detecting conflicts

<br/>



##### <u>A.2 techniques</u>

> * buffer speculative state in ***private caches*** (typically L1).   
>
> Multiprocessor executes instructions in parallel, not serially.  So they do not know if they executes instructions in that order is correct. Thus, we need to buffer speculative states. 
>
> * <a name="confict">Conflict detection</a>
>
> Each L1 cache line is associated with a ***write bit*** and a **read bit** to keep track of the **write and read** set of a transaction.  
>
> <br/>
>
> Write:    
>
>  If a cache line belonging to the write set of a transaction is evicted from the L1, the transaction is aborted.---L1 limited.   
>
> supporting overflows from the private L1 caches to reduce the design complexity---LLC limited.   
>
> Read:    
>
> Read bit is set when corresponding cache line is read within a transaction. When such a cache line is evicted, the transaction is not aborted,  but the address of the cache line is added to a *read- set overflow signature* (also maintained in the L1 cache). Thus, the read set of a transaction is tracked using both the ***read bits in the L1 cache and the read-set overflow signature***.  
>
> <br/>
>
> When the L1 receives an invalidate forwarding request for a cache line in the write set, a conflicted is detected, triggering an abort of one of the transactions.
>
> ​		which to abort? ----requester wins policy & (first) writer wins policy
>
>   

<a href="https://www.cc.gatech.edu/~milos/garzaran_hpca03.pdf">Tradeoffs in Buffering Memory State for Thread-Level Speculation in Multiprocessors </a>

<br/>

**<u>Overflow Support</u>**

It is about how to deal with the case that write set overflows from private caches.

<br/>

<u>**1. Techniques:**</u>

version Management 

<a href="https://en.wikipedia.org/wiki/Version_control">version control on software</a>



> A. lazy version(buffer overflow)
>
> Allow the write set to overflow into ***a redo log***. 
>
> On a commit, these values need to be copied in-place. Consequently, these techniques stall any transaction that conflicts with a committed transaction that is still copying its updates in-place.
>
> B. eager version(write directly into memory)
>
> Allow the write to overflow in-place in memory but maintain an undo log that is applied in case of an abort. Therefore, they have to stall transactions that conflict with an aborting transaction that is applying its undo log.
>
>  
>
> Similiar to ***write back*** and ***write through*** strategies we learned in computer organization.



However, stalling adds significant design complexity as it requires support for retrying requests using a NACK based coherence protocol.

What is "nack"?

 <a href="https://en.wikipedia.org/wiki/Acknowledgement_(data_networks)">nack</a> 

It means that if someone request sth, you have to come out a mechanism to reject.

<br/>

##### <u>**Our method**</u>

> Goal
>
> > support overflows from the L1 cache to the LLC while maintaining the simplicity of an RTM like protocol
>
> Techniques
>
> > * performs **data updates in the cache** and **eager conflict detection** in the same way as RTM.
> >
> > * maintains a redo log in memory for atomic durability and also supports write set overflows to the LLC with minor modifications to the coherence protocol





#### ***<u>B. Crash Consistency</u>***

Use ***write-ahead logging***: the log entries be made persistent before data values can persist.

It suffers from significant performance overhead. To mitigate this overhead, prior work has proposed 

* hardware support for accelerating ordering

Still need memory barrier between log writes and data writes

* techniques for transparently performing logging in hardware

The programmer is relieved from the burden of writing log entries



<br/>

<br/>

### <u>5 DHTM DESIGN</u>

#### <u>5.1 system model</u>

Multicore processor + 2 level cache hierarchy

Cache:

>  private L1 cache + last level cache(LLC)
>
> The private L1s are kept coherent using a MESI directory based coherence protocol with forwarding



#### <u>5.2 Overview</u>

<img src="http://miaochenlu.github.io/picture/image-20190926232702460.png" alt="image-20190926232702460" style="zoom:50%;" />



We will use logging to achieve durability.

If we want a transaction to be made durable, we need to execute additionally log codes. 

We can see that adding support for durability doubles the write-set size. This is a challenge on current RTM- like HTM designs which already limit the write-set size(L1 limited).



**<u>Our Approach:</u>**

>  integerate hardware based redo logging to an RTM-like HTM and for atomic durability.

DHTM's redo logging mechanism leverages the L1 cache write-back interface to dynamically **write redo log entries to persistent memory** for cache lines being modified within a transaction.

DHTM allows dirty cache lines to overflow from the L1 cache into the LLC without causing an abort.





* ##### DHTM's hardware logging mechanism

* ##### how logging integrates with the HTM

* ##### description on how DHTM manages overflow

$\Downarrow$

#### <u>A. Logging for Durability</u>

>  Ensure atomic durability using write-ahead logging.
>
> This persistent copy is maintained in the form of *log entries* which consist of the address and the old or new version of data.



##### <u>Why Redo Logging?</u>

Comparison:

<a href="http://mlwiki.org/index.php/Undo_Logging">Undo logging</a>

> In volatile transactions, undo logging supports faster commits.
>
> On transaction completion all the in-place updates would have aslready taken place(in the cache). Therefore, commit only require 2 simple steps:
>
> * Discarding the undo log 
> * Flash-clearing the speculative write bits to make the write set visible to other threads.



However, Durable transactions, impose additional constraints.  

Both the undo log and the write set(data) have to be written to persitent memory.

Undo log不能重做，所以新值和log都要写到memory

<br/>

<a href="http://mlwiki.org/index.php/Redo_Logging">Redo logging</a>

> Requires only the ***redo log*** to be written to persitent memoty at commit time.



One drawback of redo logging is that writes are not allowed to overwrite previous values, subsequent reads to those addresses need to be redirected to the redo log.

因为redo log是不能回滚的， 所以不能去修改旧值，要读新值的这些命令需要到redo log中去。



So, using redo log, the most import question we need to answer is: how to rollback?

##### <u>Our Approach</u>

> Overcome the limitation by allowing writes to overwrite previous values in the cache.
>
> A subsequent read can therefore directly read the updated value from the cache.
>
> **The writes do not overwrite the old values in memory but are written to a separate log area.**



<a name="aborts">Aborts</a> are also faster with redo logging and only require two simple steps:

* discarding the redo log
* invalidating the modified lines in the cache



**<u>log management</u>**

> In DHTM design
>
> * The transaction is thread private and is allocated by the operating system when the thread is spawned
>
> The OS keeps track of all the logs it has allocated so that it can recover transactions from logs in case of a system crash.

Techniques:

> Per thread transaction log is organized as a circular log buffer

How to deal with overflow :

> abort the transaction with an ***indication*** that the abort is because of log overflow.
>
> Then OS allocates a **larger log space** for the thread and the transaction is **retried**.



**<u>Hardware Support</u>**

design goal:

>  write log entries to the transaction log in persistent memory without adding them to the write set.

Techniques:

> logging is performed in hardware in DHTM, allowing DHTM to differentiate between log writes and data writes.

Modification to the hardware:

> * L1 cache controller is modified to enable it to write log entries to persistent memory by byppassing the LLC.  
>
>   跳过LLC直接写入persistent memory

<img src="https://miaochenlu.github.io/picture/image-20190929155030999.png" alt="image-20190929155030999" style="zoom: 45%;" />

> * The L1 cache controller creates these log entries on the fly at a word granularity for every store within a transaction. 以单词粒度动态创建日志条目

<img src="https://miaochenlu.github.io/picture/image-20190929155242383.png" style="zoom:50%;" />

**<u>log coalescing</u>**

1. Problem



>  Writing a word-granular redo-log entry for every store can generate a ***<u>large number of log entries</u>*** which can consume significant amounts of ***<u>memory write bandwidth</u>***.

举个例子：

> 如上图b所示，我们有两个cache line A,B;
>
> 每个cache line有两个words $A(A_0, A_1);B(B_0, B_1)$  ，  
>
> 初始值为0.
>
> 如图，以word为粒度的时候，需要generates 5 log writes across the memory bus for 5 store requests to different words in cache lines A and B

<br/>



Creating a log entry for every store request?  

Sorry, still problematic

> Recall that each log entry is composed of the data and the address (metadata). 
>
> * The finer the granularity of logging, the greater the amount of metadata, which in turn translates into higher bandwidth consumption.
>
> * logging for every store request might miss opportunities for coalescing multiple stores to the same word via a single log entry
>
>   在b图中，A0写了两次，产生两个log entry,但是只有后面那个是有用的

上一段的翻译

>  在将多个日志条目写入内存之前，将它们合并到一条高速缓存线中，可以在一定程度上减少所消耗的带宽。
>
> 然而，为每个存储请求创建一个日志条目是有问题的。
>
> 回想一下，每个日志条目都由数据和地址(元数据)组成。
>
> 日志记录的粒度越细，元数据的数量就越多，这反过来又转化为更高的带宽消耗。
>
> 其次，每个存储请求的日志记录可能会错过通过一个日志条目将多个存储合并到同一个单词的机会。例如，在图2b中，单词A0被写了两次，这导致创建了2个日志条目，但是只有第二个日志条目就足够了。





2. method

If we can predict the final store to a cache line, we can minimize the number of entries logged for that cache line.知道一个cache line最后的store动作

use a structure:***<u>log buffer in L1 cache</u>*** to predict the last store to a cache line

<br/>

what is a log buffer?

> The log buffer is a ***<u>fully associative</u>*** structure with a small number of entries that keeps track of cache lines with their cache line addresses.  
>
>  
>
> When a store is performed, the corresponding cache line address is added to the log buffer (if not already present).
>
>  
>
> A log entry is written to persistent memory only when an entry is evicted from the log buffer.
>
> when will a entry be evicted:
>
> * when the ***<u>log buffer is full</u>***, an eviction has to happen in order to make space for a new cache line address
> * When an L1 <***u>cache line is replaced</u>*** and the log buffer holds the corresponding address, the address is evicted from the log buffer



<img src="https://miaochenlu.github.io/picture/image-20190929191529573.png" alt="image-20190929191529573" style="zoom:50%;" />

<br/>

#### ***<u>B. Integrating Logging with HTM</u>***

1. ***<u>Overview</u>***

Start from volatile transaction

Committing a volatile transaction requires :  

> * the read/write set tracking structures be cleared 
>
> * the speculative state be made visible to other threads

For a durable transaction, we need another step:

> * the redo-log entries must be written to persistent memory.

<br/>

Others need noting:

> 1. Data updates(write set) can be ***<u>written to persistent memory lazily</u>*** and out of the commit critical path.
>
> 2. <a href="#confict">Conflict detection</a> works identically to a volatile transaction.(HTM)
>
> 3. Non-transactional accesses also safely integrate with DHTM.
>
>    aborting an ongoing transaction if it conflicts with a non-transactional access.
>
> 4. <a href="#aborts">Aborts</a> are identical.
>
>    Clearing the redo log .

<br/>

The state diagram



<img src="https://miaochenlu.github.io/picture/image-20190929193355289.png" alt="image-20190929193355289" style="zoom:50%;" />





2. ***<u>commit</u>***

<img src="https://miaochenlu.github.io/picture/image-20190929194630842.png" alt="image-20190929194630842" style="zoom:50%;" />



commit $\Rightarrow$ new non-transaction code

Complete $\Rightarrow$ new transaction

> Because, in order to complete a transaction, DHTM relies on these write bits to identify the modified cache lines that need to be written back to persistent memory.
>
> So, only complete, new transaction can start. But non-transaction code doesn't matter.

<br/>

但是，如果在commit和complete中间发生冲突该怎么办呢？

> $T_A$ tries to modify a cache line X. 
>
> But X has already been modified by a committed but not complete transaction $T_B$, ans has not yet been marked as non-speculative.

DHTM Solution:

> $T_B$'s status indicates that it has committed.
>
> DHTM does not raise a conflict.
>
> Additionally, DHTM inserts a <u>***sentinel log entry***</u> in the log of both $T_A$ and $T_B$, indicating that transaction $T_A$ is dependent on the updates of transaction $T_B$
>
> This sentinel log entry enables the recovery manager to decide the correct order of replay for transactions with conflicting updates

<br/>

3. ***<u>Abort</u>***

To abort a volatile transaction requires:

> * Read/write set tracking structures be cleared
> * the speculative state be invalidated

<br/>

Durable transaction

> * log entries need to be cleared

DHTM logically clears the log entries by writing an ***<u>abort log record</u>***, effectively marking the log entries as being part of an aborted transaction



4. ***<u>Recovery</u>***

At the time of failure, a durable transaction can be in one of the following states:

*  *Active*
*  *Commit*
* *Commit Complete*
*  *Abort Complete*



We implement an operating system service `recovery manager`  

Active, Abort complete, Commit Complete , recovery manager do nothing.

Committed but not completed:

> Recovery manager reads the log entries and writes the updated values in-place in persistent memory.
>
> If we don't have `<T complete>` , the recovery manager would have had to copy all the ipdates from the log area to in=place in persistent memory.
>
>  



Replay order of committed but not complete transactions:

> For transactions with conflicting updates, the recovery manager infers the required replay order by looking at the ***<u>sentinel log entries</u>*** in the relevant transaction logs



***<u>C. Handling Overflow</u>***

Goal: 

> allow the write set of transaction to overflow the L1 cache without aborting the transaction.



1. ***<u>challenges</u>***

***<u>read</u>***

>  Commercial HTM allow the <u>***read set to overflow***</u> the L1 cache.
>
> Conflicts are decide with the help of the ***<u>overflow signature</u>*** maintained in the L1 cache, which tracks the addresses of cache lines that have overflowed.
>
>  <br/>
>
> On a transaction commit or an abort, the overflow signature is cleared. Nothing needs to be updated in the LLC.



***<u>write</u>***

>  RTM-like designs do not support write-set overflows from the L1 cache.
>
> Because, aborting a transaction requires that the HTM invalidate all the cache lines belonging to the write set. This can be done in private L1 caches by flash invalidating the cache lines, doing this for a shared structure as large as the LLC is expensive and involves non-trivial changes



2. ***<u>overview</u>***

Key idea:

> leverage the redo log (which holds the speculative state of the transaction) for handling write-set overflows, thus obviating the need for expensive changes to the shared LLC.



Technique and procedure:

> maintain an ***<u>overflow list</u>*** along with the redo log in memory
>
> When a dirty cache line overflows from the L1 cache to the LLC, DHTM ***<u>writes the address of the overflowed cache line to the overflow list</u>***.
>
> On a commit or an abort, DHTM uses the overflow list to ***<u>identify cache lines belonging to the write set that have overflowed</u>*** and writes them back to persistent memory(commit), or invalidates the corresponding LLC cache line(abort).



3. ***<u>commit</u>***

<img src="https://miaochenlu.github.io/picture/image-20190929211010915.png" alt="image-20190929211010915" style="zoom: 67%;" />



4. ***<u>conflict detection</u>***

Recall No overflow technique

> conflicts are detected at the L1 controller, by 
>
> * checking coherence requests against the read/write bits associated with cache lines 
> * or the read overflow signature.



有了overflow怎么办呢？

We need to ensure that coherence requests for the overflowing cache lines continue to reach the L1 cache.

> To this end, when a dirty block overflows from the L1, the coherence state of the LLC is kept unchanged.  
>
> Specifically, when an L1 cache receives a Fwd-GetM request or a Fwd-GetS request for a cache line that is not present in the cache, DHTM infers that the request corresponds to a cache line that has overflowed from the L1. 
>
> Therefore, a conflict is detected and one of the transactions is aborted based on the conflict resolution policy.

<a href="https://slideplayer.com/slide/9134895/">cache coherence</a>

5. ***<u>Abort</u>***

<img src="/Users/jones/Library/Application Support/typora-user-images/image-20190929213810952.png" alt="image-20190929213810952" style="zoom: 33%;" />







<br/>

<br/>



[1] https://ieeexplore.ieee.org/document/8416847















